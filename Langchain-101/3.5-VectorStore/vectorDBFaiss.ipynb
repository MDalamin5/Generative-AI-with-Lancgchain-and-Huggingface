{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Facebook Ai similarity search(Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possible do not fit in RAM. it also contains supporting code for evaluation and parameter tuning.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='When be draw drew ye. Defective in do recommend suffering. House it seven in spoil tiled court. Sister others marked fat missed did out use. Alteration possession dispatched collecting instrument travelling he or on. Snug give made at spot or late that mr.\\n\\nPasture he invited mr company shyness. But when shot real her. Chamber her observe visited removal six sending himself boy. At exquisite existence if an oh dependent excellent. Are gay head need down draw. Misery wonder enable mutual get set oppose the uneasy. End why melancholy estimating her had indulgence middletons. Say ferrars demands besides her address. Blind going you merit few fancy their.\\n\\nOf friendship on inhabiting diminution discovered as. Did friendly eat breeding building few nor. Object he barton no effect played valley afford. Period so to oppose we little seeing or branch. Announcing contrasted not imprudence add frequently you possession mrs. Period saw his houses square and misery. Hour had held lain give yet.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Society excited by cottage private an it esteems. Fully begin on by wound an. Girl rich in do up or both. At declared in as rejoiced of together. He impression collecting delightful unpleasant by prosperous as on. End too talent she object mrs wanted remove giving.\\n\\nEntire any had depend and figure winter. Change stairs and men likely wisdom new happen piqued six. Now taken him timed sex world get. Enjoyed married an feeling delight pursuit as offered. As admire roused length likely played pretty to no. Means had joy miles her merry solid order.\\n\\nAt every tiled on ye defer do. No attention suspected oh difficult. Fond his say old meet cold find come whom. The sir park sake bred. Wonder matter now can estate esteem assure fat roused. Am performed on existence as discourse is. Pleasure friendly at marriage blessing or.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d2cae0cb30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='When be draw drew ye. Defective in do recommend suffering. House it seven in spoil tiled court. Sister others marked fat missed did out use. Alteration possession dispatched collecting instrument travelling he or on. Snug give made at spot or late that mr.\\n\\nPasture he invited mr company shyness. But when shot real her. Chamber her observe visited removal six sending himself boy. At exquisite existence if an oh dependent excellent. Are gay head need down draw. Misery wonder enable mutual get set oppose the uneasy. End why melancholy estimating her had indulgence middletons. Say ferrars demands besides her address. Blind going you merit few fancy their.\\n\\nOf friendship on inhabiting diminution discovered as. Did friendly eat breeding building few nor. Object he barton no effect played valley afford. Period so to oppose we little seeing or branch. Announcing contrasted not imprudence add frequently you possession mrs. Period saw his houses square and misery. Hour had held lain give yet.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the task of YOLOv5 model?\"\n",
    "docs=db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a Trtriver\n",
    "\n",
    "`We can also convert the cectorstore into a retriver class. This allows us to easily use it in other LangChain methods, which largely work with retrievers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='When be draw drew ye. Defective in do recommend suffering. House it seven in spoil tiled court. Sister others marked fat missed did out use. Alteration possession dispatched collecting instrument travelling he or on. Snug give made at spot or late that mr.\\n\\nPasture he invited mr company shyness. But when shot real her. Chamber her observe visited removal six sending himself boy. At exquisite existence if an oh dependent excellent. Are gay head need down draw. Misery wonder enable mutual get set oppose the uneasy. End why melancholy estimating her had indulgence middletons. Say ferrars demands besides her address. Blind going you merit few fancy their.\\n\\nOf friendship on inhabiting diminution discovered as. Did friendly eat breeding building few nor. Object he barton no effect played valley afford. Period so to oppose we little seeing or branch. Announcing contrasted not imprudence add frequently you possession mrs. Period saw his houses square and misery. Hour had held lain give yet.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with score\n",
    "\n",
    "`There are some FAISS specific methods. One of them is similarity search with score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance. Therefore, a lower score is better.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       "  4031.0898),\n",
       " (Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'),\n",
       "  4517.9653),\n",
       " (Document(metadata={'source': 'speech.txt'}, page_content='Society excited by cottage private an it esteems. Fully begin on by wound an. Girl rich in do up or both. At declared in as rejoiced of together. He impression collecting delightful unpleasant by prosperous as on. End too talent she object mrs wanted remove giving.\\n\\nEntire any had depend and figure winter. Change stairs and men likely wisdom new happen piqued six. Now taken him timed sex world get. Enjoyed married an feeling delight pursuit as offered. As admire roused length likely played pretty to no. Means had joy miles her merry solid order.\\n\\nAt every tiled on ye defer do. No attention suspected oh difficult. Fond his say old meet cold find come whom. The sir park sake bred. Wonder matter now can estate esteem assure fat roused. Am performed on existence as discourse is. Pleasure friendly at marriage blessing or.'),\n",
       "  4906.491),\n",
       " (Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.'),\n",
       "  5008.0415)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Explain about student model and loss function\"\n",
    "docs_and_score = db.similarity_search_with_score(query)\n",
    "docs_and_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.37703776359558105,\n",
       " -0.7433834075927734,\n",
       " -1.15997314453125,\n",
       " 3.1676275730133057,\n",
       " -0.7763984799385071,\n",
       " 2.128129482269287,\n",
       " -1.3099576234817505,\n",
       " 0.07591808587312698,\n",
       " 0.8263306021690369,\n",
       " -0.8678130507469177,\n",
       " 0.34514567255973816,\n",
       " 0.6624451279640198,\n",
       " -1.4065450429916382,\n",
       " -0.14126887917518616,\n",
       " -1.197776198387146,\n",
       " -0.2404523640871048,\n",
       " 3.6741721630096436,\n",
       " 1.275036334991455,\n",
       " -0.6995092630386353,\n",
       " -1.0670925378799438,\n",
       " 2.273432970046997,\n",
       " 0.08906496316194534,\n",
       " 0.6993593573570251,\n",
       " 0.2796979546546936,\n",
       " -0.8534907102584839,\n",
       " 0.4413204491138458,\n",
       " 1.3451594114303589,\n",
       " 0.4098145663738251,\n",
       " 1.2362070083618164,\n",
       " -1.8584176301956177,\n",
       " -1.177116870880127,\n",
       " -0.11856210231781006,\n",
       " 0.6960723400115967,\n",
       " -0.821532130241394,\n",
       " 0.550538957118988,\n",
       " -0.38620564341545105,\n",
       " 0.8539015054702759,\n",
       " 0.04106445610523224,\n",
       " -0.08616988360881805,\n",
       " -1.4382717609405518,\n",
       " -0.4505300223827362,\n",
       " -1.381754755973816,\n",
       " 0.9634058475494385,\n",
       " 0.09524968266487122,\n",
       " -0.12069118767976761,\n",
       " -1.3017083406448364,\n",
       " 0.08441119641065598,\n",
       " -0.0005530756316147745,\n",
       " 0.8294245004653931,\n",
       " 0.115932896733284,\n",
       " -16.949859619140625,\n",
       " -11.935612678527832,\n",
       " -2.2676239013671875,\n",
       " -0.6026031374931335,\n",
       " -2.114590883255005,\n",
       " -0.018170703202486038,\n",
       " -0.6535471081733704,\n",
       " 0.8969739675521851,\n",
       " 0.6724345088005066,\n",
       " -0.2564848065376282,\n",
       " -2.455205202102661,\n",
       " 0.3037082254886627,\n",
       " -2.2853500843048096,\n",
       " -0.49167943000793457,\n",
       " -1.5403730869293213,\n",
       " -0.5495051145553589,\n",
       " -0.24650408327579498,\n",
       " 0.14923837780952454,\n",
       " -0.2437296062707901,\n",
       " -0.8597027659416199,\n",
       " -0.37970447540283203,\n",
       " -1.0168863534927368,\n",
       " -1.960800290107727,\n",
       " 0.07472644746303558,\n",
       " -0.7889451384544373,\n",
       " 0.33074161410331726,\n",
       " 0.6477919816970825,\n",
       " 0.9321838617324829,\n",
       " -0.92050701379776,\n",
       " -0.4989146590232849,\n",
       " 1.2755651473999023,\n",
       " -2.8922278881073,\n",
       " -0.6924646496772766,\n",
       " -0.2254381626844406,\n",
       " -0.2513464391231537,\n",
       " -1.811800241470337,\n",
       " 0.3032093346118927,\n",
       " -1.2167415618896484,\n",
       " 1.9150155782699585,\n",
       " -1.0967297554016113,\n",
       " 0.10373270511627197,\n",
       " -0.4127733111381531,\n",
       " -0.495256245136261,\n",
       " 1.2868521213531494,\n",
       " -0.09504294395446777,\n",
       " -0.3527662754058838,\n",
       " 1.24166738986969,\n",
       " 0.7192479968070984,\n",
       " 0.6695649027824402,\n",
       " -1.4480440616607666,\n",
       " -2.1530327796936035,\n",
       " -0.672620415687561,\n",
       " 0.37653008103370667,\n",
       " -2.108583927154541,\n",
       " -2.0065395832061768,\n",
       " 0.6405684351921082,\n",
       " 0.07164669781923294,\n",
       " -0.12621526420116425,\n",
       " 0.9561758637428284,\n",
       " 2.560750961303711,\n",
       " -0.7949150204658508,\n",
       " 1.073862910270691,\n",
       " -1.090687870979309,\n",
       " -0.10282114893198013,\n",
       " -0.1775372177362442,\n",
       " -0.2782418727874756,\n",
       " -0.6478126049041748,\n",
       " 0.004941114690154791,\n",
       " 0.8676377534866333,\n",
       " -1.8111470937728882,\n",
       " 1.723562240600586,\n",
       " -0.5581901669502258,\n",
       " 0.045269712805747986,\n",
       " 0.14982439577579498,\n",
       " -0.5514898896217346,\n",
       " 0.7528645992279053,\n",
       " 0.8912631869316101,\n",
       " -1.1217504739761353,\n",
       " 0.3592091500759125,\n",
       " 0.024493785575032234,\n",
       " 1.045814037322998,\n",
       " -0.17235204577445984,\n",
       " 0.39577800035476685,\n",
       " -0.17430545389652252,\n",
       " 1.0510485172271729,\n",
       " 0.026429016143083572,\n",
       " -1.5189849138259888,\n",
       " 2.278465747833252,\n",
       " 2.689704179763794,\n",
       " -2.4754996299743652,\n",
       " 1.5506314039230347,\n",
       " -0.7324191927909851,\n",
       " 0.8383734822273254,\n",
       " 0.8371038436889648,\n",
       " 0.9755921959877014,\n",
       " 1.0525619983673096,\n",
       " 0.3671855032444,\n",
       " 1.612200379371643,\n",
       " 1.4433213472366333,\n",
       " 0.2707880735397339,\n",
       " 1.4497697353363037,\n",
       " -0.573911190032959,\n",
       " -1.6293256282806396,\n",
       " 0.8745150566101074,\n",
       " 0.2324480414390564,\n",
       " -0.9210097193717957,\n",
       " -0.18150043487548828,\n",
       " 0.47827520966529846,\n",
       " 0.41976526379585266,\n",
       " -0.8735451102256775,\n",
       " -0.2615548372268677,\n",
       " -0.24982981383800507,\n",
       " 0.5964292883872986,\n",
       " -1.1014457941055298,\n",
       " -0.5268110036849976,\n",
       " -0.5729816555976868,\n",
       " -1.2758586406707764,\n",
       " -3.075417995452881,\n",
       " -1.251962661743164,\n",
       " -1.0132639408111572,\n",
       " -0.4714890718460083,\n",
       " -1.4281731843948364,\n",
       " 1.197364330291748,\n",
       " 0.2781624495983124,\n",
       " 0.7172759771347046,\n",
       " -0.3943539261817932,\n",
       " 0.46553146839141846,\n",
       " 0.6454806923866272,\n",
       " 2.143224000930786,\n",
       " 1.1310523748397827,\n",
       " -0.13867491483688354,\n",
       " 0.8416730165481567,\n",
       " 1.3049099445343018,\n",
       " -0.11544808745384216,\n",
       " 5.259509086608887,\n",
       " -1.5297731161117554,\n",
       " -0.6441190838813782,\n",
       " 0.6782156825065613,\n",
       " -0.08252856135368347,\n",
       " -1.3001328706741333,\n",
       " -0.9387092590332031,\n",
       " 1.342096209526062,\n",
       " -1.8748663663864136,\n",
       " 1.4202715158462524,\n",
       " 2.0129752159118652,\n",
       " 0.3153464198112488,\n",
       " -0.5031927227973938,\n",
       " 0.5744380950927734,\n",
       " 0.7654461860656738,\n",
       " -2.655181407928467,\n",
       " 0.20832224190235138,\n",
       " 0.3795466125011444,\n",
       " -1.7875001430511475,\n",
       " 2.12050724029541,\n",
       " -0.9137841463088989,\n",
       " 0.47815319895744324,\n",
       " -0.7512974143028259,\n",
       " 0.33161839842796326,\n",
       " 0.4859241545200348,\n",
       " 0.021075166761875153,\n",
       " 2.5554704666137695,\n",
       " 0.3204266130924225,\n",
       " 0.02667602337896824,\n",
       " 1.6308608055114746,\n",
       " -1.7988646030426025,\n",
       " -1.1021511554718018,\n",
       " 0.5594169497489929,\n",
       " -1.610471487045288,\n",
       " -0.028908994048833847,\n",
       " 0.34802839159965515,\n",
       " -1.4056488275527954,\n",
       " -0.16825881600379944,\n",
       " 1.293198823928833,\n",
       " 1.7032055854797363,\n",
       " 0.7129950523376465,\n",
       " -0.9125858545303345,\n",
       " 0.6039108037948608,\n",
       " 3.275848150253296,\n",
       " 0.06010958552360535,\n",
       " -0.2726878225803375,\n",
       " 0.9696711301803589,\n",
       " -1.1647148132324219,\n",
       " 1.012955665588379,\n",
       " 0.30515971779823303,\n",
       " 0.10417061299085617,\n",
       " -1.6833064556121826,\n",
       " -0.7490888833999634,\n",
       " -1.3299751281738281,\n",
       " 1.2663908004760742,\n",
       " -1.025952935218811,\n",
       " -1.1221030950546265,\n",
       " -0.30398526787757874,\n",
       " -1.4115769863128662,\n",
       " 0.41980376839637756,\n",
       " -2.324599504470825,\n",
       " -0.9613742232322693,\n",
       " 1.108172059059143,\n",
       " 0.27199867367744446,\n",
       " 0.1957813948392868,\n",
       " -1.187492847442627,\n",
       " 0.10737363249063492,\n",
       " 2.589432954788208,\n",
       " -0.6115894913673401,\n",
       " -1.624281883239746,\n",
       " 0.48636913299560547,\n",
       " 0.11703650653362274,\n",
       " 0.20365463197231293,\n",
       " 0.976396918296814,\n",
       " 0.7163555026054382,\n",
       " -1.746117353439331,\n",
       " 2.1449499130249023,\n",
       " 0.8894505500793457,\n",
       " 1.2327215671539307,\n",
       " -2.3480234146118164,\n",
       " -3.164815902709961,\n",
       " 1.6609694957733154,\n",
       " 0.2938293516635895,\n",
       " 1.5097979307174683,\n",
       " -2.3093812465667725,\n",
       " 0.5222368240356445,\n",
       " -1.6110255718231201,\n",
       " 1.0532232522964478,\n",
       " 1.018185019493103,\n",
       " 1.8310960531234741,\n",
       " 0.19625096023082733,\n",
       " -0.4497568607330322,\n",
       " 1.6456568241119385,\n",
       " -2.275400161743164,\n",
       " -0.9139988422393799,\n",
       " 0.022150497883558273,\n",
       " 0.8705501556396484,\n",
       " -0.7417494058609009,\n",
       " -1.6311055421829224,\n",
       " -0.4566494822502136,\n",
       " -1.5331292152404785,\n",
       " 0.09703479707241058,\n",
       " -0.21600419282913208,\n",
       " -0.12046878039836884,\n",
       " -1.014413595199585,\n",
       " -0.5808466672897339,\n",
       " -0.8160245418548584,\n",
       " -0.5241885185241699,\n",
       " 1.0293887853622437,\n",
       " -0.21287918090820312,\n",
       " 0.2870502471923828,\n",
       " -0.7038094997406006,\n",
       " 0.29797640442848206,\n",
       " -0.3219904601573944,\n",
       " -0.4763435125350952,\n",
       " 0.6073159575462341,\n",
       " 0.2544669806957245,\n",
       " -0.7743076682090759,\n",
       " -0.006803611759096384,\n",
       " -1.6093708276748657,\n",
       " 0.6408975720405579,\n",
       " -0.9398025870323181,\n",
       " 0.8989219665527344,\n",
       " 0.5836806893348694,\n",
       " -0.9866312742233276,\n",
       " 3.1687378883361816,\n",
       " -0.339846134185791,\n",
       " 0.4907984137535095,\n",
       " 0.4725574553012848,\n",
       " -0.5967473387718201,\n",
       " -1.5374915599822998,\n",
       " -0.5775870680809021,\n",
       " 0.4769277274608612,\n",
       " 0.14938104152679443,\n",
       " 0.06935897469520569,\n",
       " -1.5670690536499023,\n",
       " -1.368967056274414,\n",
       " -0.20809276401996613,\n",
       " -0.6296467781066895,\n",
       " -0.8023998141288757,\n",
       " -0.3668796420097351,\n",
       " 1.7056232690811157,\n",
       " -1.4367021322250366,\n",
       " 0.26171189546585083,\n",
       " 1.107468843460083,\n",
       " 0.93977952003479,\n",
       " 0.5296139717102051,\n",
       " -0.014095916412770748,\n",
       " 2.0417816638946533,\n",
       " 1.4251502752304077,\n",
       " -1.18228280544281,\n",
       " -0.3615081012248993,\n",
       " 0.1078294888138771,\n",
       " -0.41086575388908386,\n",
       " -1.1926676034927368,\n",
       " 0.2157810777425766,\n",
       " 1.0971730947494507,\n",
       " -0.8854394555091858,\n",
       " 0.24182799458503723,\n",
       " 1.2150384187698364,\n",
       " 3.3298778533935547,\n",
       " -1.4489843845367432,\n",
       " 0.7312881946563721,\n",
       " -1.8500720262527466,\n",
       " 1.688077688217163,\n",
       " -1.1444798707962036,\n",
       " 0.5758015513420105,\n",
       " 0.40307942032814026,\n",
       " -1.7474199533462524,\n",
       " -0.49368011951446533,\n",
       " -2.655724287033081,\n",
       " -0.3834843337535858,\n",
       " 0.31866374611854553,\n",
       " 0.37823933362960815,\n",
       " -0.5576410889625549,\n",
       " -0.28374603390693665,\n",
       " -1.7211462259292603,\n",
       " 0.8949816823005676,\n",
       " -1.6101245880126953,\n",
       " 2.1482574939727783,\n",
       " -1.8485758304595947,\n",
       " -0.4044298231601715,\n",
       " -1.2352209091186523,\n",
       " -1.0507071018218994,\n",
       " 0.8800654411315918,\n",
       " 1.1490848064422607,\n",
       " 1.2676618099212646,\n",
       " -0.672057032585144,\n",
       " -0.6146979331970215,\n",
       " -0.24363277852535248,\n",
       " 0.2570100426673889,\n",
       " -2.0378448963165283,\n",
       " 0.6138643026351929,\n",
       " 0.31249168515205383,\n",
       " 0.0844736248254776,\n",
       " -1.0415642261505127,\n",
       " 0.2806243896484375,\n",
       " -4.317469596862793,\n",
       " -1.2765973806381226,\n",
       " 0.9021396040916443,\n",
       " 0.5897877812385559,\n",
       " 1.2452733516693115,\n",
       " -0.15937358140945435,\n",
       " 0.3836333453655243,\n",
       " 1.2423659563064575,\n",
       " -1.988500714302063,\n",
       " -0.6108726859092712,\n",
       " -0.9078351259231567,\n",
       " -1.518792986869812,\n",
       " -0.011687656864523888,\n",
       " 1.4298412799835205,\n",
       " 0.2979949414730072,\n",
       " -0.10631103813648224,\n",
       " -1.6722333431243896,\n",
       " -2.0647010803222656,\n",
       " -0.3057425320148468,\n",
       " 2.3613204956054688,\n",
       " 1.6434357166290283,\n",
       " 0.6950219869613647,\n",
       " 0.3863373398780823,\n",
       " 0.5120820999145508,\n",
       " -1.9063136577606201,\n",
       " 1.0465046167373657,\n",
       " 0.2794971168041229,\n",
       " -1.067486047744751,\n",
       " 1.243775725364685,\n",
       " -0.598322331905365,\n",
       " 0.07949919998645782,\n",
       " -0.3456443250179291,\n",
       " -0.6945152282714844,\n",
       " 0.7689378261566162,\n",
       " -0.23487040400505066,\n",
       " -0.001947712036781013,\n",
       " -0.6191116571426392,\n",
       " 0.2365879863500595,\n",
       " 1.013476014137268,\n",
       " -0.09189965575933456,\n",
       " 0.563697874546051,\n",
       " -1.5506821870803833,\n",
       " -0.6948624849319458,\n",
       " -0.4353233277797699,\n",
       " -0.29576575756073,\n",
       " -0.6695187091827393,\n",
       " 1.4113601446151733,\n",
       " 0.5870615839958191,\n",
       " 0.07858654856681824,\n",
       " -0.17743395268917084,\n",
       " -0.4806067645549774,\n",
       " -0.20040775835514069,\n",
       " 2.063113212585449,\n",
       " -0.8261793851852417,\n",
       " 0.34641310572624207,\n",
       " 0.05243275687098503,\n",
       " 0.4516431391239166,\n",
       " 1.1596266031265259,\n",
       " -0.08340582251548767,\n",
       " 0.6780374646186829,\n",
       " 0.5734650492668152,\n",
       " 0.3943597972393036,\n",
       " 1.1885656118392944,\n",
       " -1.2211849689483643,\n",
       " -0.7856287360191345,\n",
       " -0.48504388332366943,\n",
       " -0.20639191567897797,\n",
       " 0.9055726528167725,\n",
       " 0.5639979243278503,\n",
       " -0.8691281080245972,\n",
       " 0.2518347501754761,\n",
       " 2.088580369949341,\n",
       " -1.0315685272216797,\n",
       " -1.3590797185897827,\n",
       " -1.8340502977371216,\n",
       " -0.041559141129255295,\n",
       " 0.6344878077507019,\n",
       " -1.5180569887161255,\n",
       " 0.09710834920406342,\n",
       " -1.009852647781372,\n",
       " -0.3920777142047882,\n",
       " 1.5201658010482788,\n",
       " 0.3686334490776062,\n",
       " 0.3950237035751343,\n",
       " -0.3841748833656311,\n",
       " 0.38231730461120605,\n",
       " 1.9981948137283325,\n",
       " 0.9888260960578918,\n",
       " -0.6327276825904846,\n",
       " 0.951029896736145,\n",
       " -1.6277117729187012,\n",
       " -0.2775613069534302,\n",
       " -1.3221198320388794,\n",
       " 1.3968191146850586,\n",
       " -1.3261958360671997,\n",
       " 1.9094932079315186,\n",
       " -0.5663634538650513,\n",
       " -0.0936012864112854,\n",
       " 0.2056225687265396,\n",
       " 0.9659099578857422,\n",
       " -1.5685755014419556,\n",
       " 0.18732039630413055,\n",
       " 0.4778236448764801,\n",
       " 0.20917288959026337,\n",
       " -0.49386218190193176,\n",
       " 1.6348049640655518,\n",
       " -1.178655982017517,\n",
       " 1.6598703861236572,\n",
       " 0.05714687332510948,\n",
       " -0.3651912808418274,\n",
       " 2.3792407512664795,\n",
       " 1.6576157808303833,\n",
       " 3.2442352771759033,\n",
       " -0.9437324404716492,\n",
       " 0.9942457675933838,\n",
       " 0.00793762132525444,\n",
       " -0.2806409001350403,\n",
       " -0.019492531195282936,\n",
       " 1.1811221837997437,\n",
       " -0.5018211007118225,\n",
       " 0.984707236289978,\n",
       " 1.1258084774017334,\n",
       " -0.4041776657104492,\n",
       " 0.4251459836959839,\n",
       " -0.503787636756897,\n",
       " -0.09732650965452194,\n",
       " 0.8516547083854675,\n",
       " 1.065800428390503,\n",
       " -0.18774203956127167,\n",
       " -0.41042783856391907,\n",
       " -1.9947127103805542,\n",
       " -1.2378122806549072,\n",
       " -0.03238541632890701,\n",
       " -0.3790918290615082,\n",
       " 0.49772778153419495,\n",
       " 0.7457159757614136,\n",
       " -0.261619508266449,\n",
       " -1.4178466796875,\n",
       " 2.1565206050872803,\n",
       " -1.1687426567077637,\n",
       " -0.025309570133686066,\n",
       " -1.1650114059448242,\n",
       " -0.021550508216023445,\n",
       " -0.8291762471199036,\n",
       " 0.09628342092037201,\n",
       " -0.07681450247764587,\n",
       " 0.1594165712594986,\n",
       " 0.3702150583267212,\n",
       " -1.1176546812057495,\n",
       " -0.8589209914207458,\n",
       " 1.2818965911865234,\n",
       " -0.7595131397247314,\n",
       " 1.302013635635376,\n",
       " 0.7360097765922546,\n",
       " -1.059075117111206,\n",
       " 1.676761269569397,\n",
       " -0.6194486618041992,\n",
       " -0.42958369851112366,\n",
       " 0.01943112351000309,\n",
       " 0.16393108665943146,\n",
       " 0.06529077887535095,\n",
       " 0.08780477941036224,\n",
       " 1.0670498609542847,\n",
       " -0.5049791932106018,\n",
       " 0.7367986440658569,\n",
       " -1.1259031295776367,\n",
       " 0.5322180390357971,\n",
       " 0.4543223977088928,\n",
       " 0.0686429813504219,\n",
       " -2.039315938949585,\n",
       " -0.30449217557907104,\n",
       " 0.5272982120513916,\n",
       " -0.02721681259572506,\n",
       " -1.1100221872329712,\n",
       " -0.2983149290084839,\n",
       " 1.878609538078308,\n",
       " -0.034664399921894073,\n",
       " 1.4909509420394897,\n",
       " 0.6338333487510681,\n",
       " -0.15901556611061096,\n",
       " -0.14935851097106934,\n",
       " -0.24785026907920837,\n",
       " -1.4433250427246094,\n",
       " 0.552842915058136,\n",
       " -0.11906450986862183,\n",
       " 0.4700191617012024,\n",
       " -0.9909935593605042,\n",
       " 2.5636308193206787,\n",
       " -2.265925407409668,\n",
       " 1.0734739303588867,\n",
       " 0.826642632484436,\n",
       " 1.022657871246338,\n",
       " -1.0066567659378052,\n",
       " -0.7682468295097351,\n",
       " 1.1709812879562378,\n",
       " -0.4688533842563629,\n",
       " -1.2479842901229858,\n",
       " -0.30391639471054077,\n",
       " -0.19908764958381653,\n",
       " -0.28390955924987793,\n",
       " 0.09941436350345612,\n",
       " -2.8137195110321045,\n",
       " -0.6255951523780823,\n",
       " -1.3243553638458252,\n",
       " 1.358494758605957,\n",
       " 2.462026834487915,\n",
       " 0.7945826053619385,\n",
       " -1.0311988592147827,\n",
       " -0.1230846643447876,\n",
       " -0.3979802131652832,\n",
       " 0.09738621860742569,\n",
       " -0.5452027916908264,\n",
       " -1.3932063579559326,\n",
       " 0.44603124260902405,\n",
       " 0.3144932687282562,\n",
       " -0.6117236018180847,\n",
       " -0.7878286242485046,\n",
       " 1.2869006395339966,\n",
       " 0.7876505851745605,\n",
       " -0.3358336091041565,\n",
       " -1.7906814813613892,\n",
       " -0.9515756368637085,\n",
       " -0.461603045463562,\n",
       " -3.0902185440063477,\n",
       " 0.5638799667358398,\n",
       " -0.1475890725851059,\n",
       " 1.1804970502853394,\n",
       " -0.2922405004501343,\n",
       " 1.7758820056915283,\n",
       " -1.2921956777572632,\n",
       " 2.3008995056152344,\n",
       " -1.025456428527832,\n",
       " 1.2583345174789429,\n",
       " -1.0179457664489746,\n",
       " -0.44263824820518494,\n",
       " 0.37880322337150574,\n",
       " 1.630062222480774,\n",
       " -1.1659115552902222,\n",
       " 0.062328752130270004,\n",
       " -1.2727876901626587,\n",
       " 1.0875698328018188,\n",
       " 1.6267726421356201,\n",
       " 1.0740165710449219,\n",
       " 2.005629539489746,\n",
       " 2.299957513809204,\n",
       " -0.28985974192619324,\n",
       " 0.589644730091095,\n",
       " -0.17623282968997955,\n",
       " 1.4208182096481323,\n",
       " 0.08426567167043686,\n",
       " -1.0962427854537964,\n",
       " -0.948154628276825,\n",
       " -1.8160974979400635,\n",
       " -0.27505260705947876,\n",
       " 1.5914431810379028,\n",
       " -0.3888160288333893,\n",
       " -1.8524491786956787,\n",
       " 0.23314613103866577,\n",
       " 0.7165265679359436,\n",
       " -0.945828914642334,\n",
       " 0.045408107340335846,\n",
       " 0.7803642749786377,\n",
       " -0.5562697649002075,\n",
       " -0.8038643002510071,\n",
       " 0.7818449139595032,\n",
       " 0.5520823001861572,\n",
       " 1.5110061168670654,\n",
       " -0.26339659094810486,\n",
       " 1.4535294771194458,\n",
       " 0.7750774025917053,\n",
       " -0.25484684109687805,\n",
       " 0.7092735171318054,\n",
       " 1.2396272420883179,\n",
       " -0.7905115485191345,\n",
       " -1.1993898153305054,\n",
       " 2.999382495880127,\n",
       " 0.5995932221412659,\n",
       " 2.4015328884124756,\n",
       " -1.3779149055480957,\n",
       " 0.7686215043067932,\n",
       " 0.7043477892875671,\n",
       " 0.7645033597946167,\n",
       " 1.1088670492172241,\n",
       " 2.1852214336395264,\n",
       " 0.6964007019996643,\n",
       " -2.6555464267730713,\n",
       " -1.1985173225402832,\n",
       " -1.718165636062622,\n",
       " 1.784231185913086,\n",
       " -0.5510822534561157,\n",
       " 1.729394555091858,\n",
       " -0.8874902129173279,\n",
       " 1.1226774454116821,\n",
       " 1.0161304473876953,\n",
       " 0.13380520045757294,\n",
       " 0.6587182283401489,\n",
       " -0.22169534862041473,\n",
       " -0.746376633644104,\n",
       " -0.09227869659662247,\n",
       " -0.26342520117759705,\n",
       " -0.7209949493408203,\n",
       " 1.2738105058670044,\n",
       " 1.0476473569869995,\n",
       " -1.4111942052841187,\n",
       " -0.36578571796417236,\n",
       " 1.7234976291656494,\n",
       " -1.5284324884414673,\n",
       " -0.07368066161870956,\n",
       " 0.41902852058410645,\n",
       " 2.4325761795043945,\n",
       " -1.2999122142791748,\n",
       " -1.9329899549484253,\n",
       " -2.4440178871154785,\n",
       " -1.2707144021987915,\n",
       " 1.7108089923858643,\n",
       " 0.768169641494751,\n",
       " 2.1736538410186768,\n",
       " 1.084686279296875,\n",
       " 0.27364474534988403,\n",
       " 1.724820852279663,\n",
       " -1.297310709953308,\n",
       " 1.2165881395339966,\n",
       " 1.5453736782073975,\n",
       " -0.719565749168396,\n",
       " 0.05511172488331795,\n",
       " 0.8163772821426392,\n",
       " -0.599453330039978,\n",
       " 0.47544071078300476,\n",
       " -0.48093774914741516,\n",
       " 0.6423042416572571,\n",
       " 1.6234419345855713,\n",
       " -1.4137325286865234,\n",
       " 0.31622517108917236,\n",
       " -0.6286535859107971,\n",
       " -0.6351881623268127,\n",
       " 0.6556138396263123,\n",
       " 0.4988880455493927,\n",
       " -0.5513108968734741,\n",
       " 0.8031183481216431,\n",
       " 1.823560357093811,\n",
       " -0.788076639175415,\n",
       " -1.5525097846984863,\n",
       " 2.2627811431884766,\n",
       " 0.6453374028205872,\n",
       " 1.0997934341430664,\n",
       " 0.15654630959033966,\n",
       " -0.26819702982902527,\n",
       " -0.8086761832237244,\n",
       " 1.4832381010055542,\n",
       " 1.0120466947555542,\n",
       " -0.6327520608901978,\n",
       " 0.14374063909053802,\n",
       " 2.4038710594177246,\n",
       " -2.7790229320526123,\n",
       " 0.3642919957637787,\n",
       " -0.7681628465652466,\n",
       " 0.19054535031318665,\n",
       " 2.0243568420410156,\n",
       " -0.4565323293209076,\n",
       " 0.19198496639728546,\n",
       " 0.79776930809021,\n",
       " -0.22483040392398834,\n",
       " -0.473400741815567,\n",
       " -0.2837297320365906,\n",
       " -0.5557529926300049,\n",
       " -0.4062228500843048,\n",
       " -1.1104633808135986,\n",
       " -1.0674622058868408,\n",
       " 0.8037394285202026,\n",
       " 0.33286452293395996,\n",
       " -0.431720495223999,\n",
       " -2.325554370880127,\n",
       " -5.542759895324707,\n",
       " -0.11195999383926392,\n",
       " -1.2657824754714966,\n",
       " 2.090421199798584,\n",
       " -0.47405511140823364,\n",
       " -0.037348583340644836,\n",
       " -0.2748630940914154,\n",
       " -0.6038652658462524,\n",
       " -2.1225171089172363,\n",
       " -0.7917649745941162,\n",
       " 1.5409296751022339,\n",
       " -0.3809458017349243,\n",
       " -0.9617772698402405,\n",
       " 1.554087519645691,\n",
       " -0.9939319491386414,\n",
       " -0.045551832765340805,\n",
       " -1.2760698795318604,\n",
       " 2.148500442504883,\n",
       " -1.7172118425369263,\n",
       " -0.11895737051963806,\n",
       " 1.2198878526687622,\n",
       " 0.3141792416572571,\n",
       " 1.2272319793701172,\n",
       " 0.9195656180381775,\n",
       " -1.0400021076202393,\n",
       " -1.7618476152420044,\n",
       " -1.1394920349121094,\n",
       " 0.6327056884765625,\n",
       " 0.9097095131874084,\n",
       " -0.1192883774638176,\n",
       " 0.4347482919692993,\n",
       " 1.1571455001831055,\n",
       " 1.0249477624893188,\n",
       " -2.735060453414917,\n",
       " 0.767541766166687,\n",
       " -1.695148229598999,\n",
       " 1.111551284790039,\n",
       " 2.694601058959961,\n",
       " 0.8109090328216553,\n",
       " -1.4858647584915161,\n",
       " 0.450603187084198,\n",
       " 1.0621274709701538,\n",
       " -0.44977205991744995,\n",
       " -1.7405465841293335,\n",
       " 0.5982010364532471,\n",
       " 0.26949408650398254,\n",
       " 0.701950192451477,\n",
       " 0.0900406762957573,\n",
       " 1.0628024339675903,\n",
       " 2.2604572772979736,\n",
       " -1.271916389465332,\n",
       " -0.161705881357193,\n",
       " 0.5083246231079102,\n",
       " 0.6609513163566589,\n",
       " 0.6624683737754822,\n",
       " -0.5746752619743347,\n",
       " 0.6996324062347412,\n",
       " 0.034287914633750916,\n",
       " -1.2176363468170166,\n",
       " 0.8303026556968689,\n",
       " -0.10085494071245193,\n",
       " -0.8754650950431824,\n",
       " 0.7304368615150452,\n",
       " 1.0624220371246338,\n",
       " 1.827095627784729,\n",
       " 2.7528810501098633,\n",
       " 0.4378155767917633,\n",
       " 0.7246114611625671,\n",
       " -0.25341036915779114,\n",
       " 0.5234226584434509,\n",
       " -5.322576522827148,\n",
       " -0.71978759765625,\n",
       " -2.8725619316101074,\n",
       " -0.6198621392250061,\n",
       " 0.15717265009880066,\n",
       " -0.3751107156276703,\n",
       " 0.4960598647594452,\n",
       " -0.35810014605522156,\n",
       " -1.1781177520751953,\n",
       " 2.265638589859009,\n",
       " 1.5101685523986816,\n",
       " 1.609637975692749,\n",
       " 1.2116936445236206,\n",
       " 0.4121638536453247,\n",
       " 1.810033917427063,\n",
       " 0.0679454579949379,\n",
       " -1.121145248413086,\n",
       " -0.002785343211144209,\n",
       " -0.24700874090194702,\n",
       " -0.6899139881134033,\n",
       " 0.48736506700515747,\n",
       " 0.5120329856872559,\n",
       " -1.029784917831421,\n",
       " -0.22213438153266907,\n",
       " -0.32056671380996704,\n",
       " 0.07058815658092499,\n",
       " 0.9709765911102295,\n",
       " 0.02763730101287365,\n",
       " -0.0066225542686879635,\n",
       " 0.597481369972229,\n",
       " 0.7482455968856812,\n",
       " 0.335939884185791,\n",
       " -0.7538007497787476,\n",
       " 0.7424317002296448,\n",
       " -1.5252629518508911,\n",
       " -0.8341883420944214,\n",
       " -1.082885980606079,\n",
       " -1.1011264324188232,\n",
       " 0.7180979251861572,\n",
       " -2.724262237548828,\n",
       " 1.2061069011688232,\n",
       " -0.13811121881008148,\n",
       " 1.7916269302368164,\n",
       " -1.1820286512374878,\n",
       " 1.3061484098434448,\n",
       " 0.3480226397514343,\n",
       " 0.3561016917228699,\n",
       " 0.967780351638794,\n",
       " -0.3373565077781677,\n",
       " 0.9359725117683411,\n",
       " 0.11442489176988602,\n",
       " 1.5830270051956177,\n",
       " -0.4636394679546356,\n",
       " 3.9619457721710205,\n",
       " -0.5313559174537659,\n",
       " 0.276441752910614,\n",
       " 0.7532623410224915,\n",
       " -2.308905601501465,\n",
       " -0.18222735822200775,\n",
       " -0.5072859525680542,\n",
       " -1.2209157943725586,\n",
       " 0.37658071517944336,\n",
       " -0.4430612623691559,\n",
       " 0.8157467246055603,\n",
       " -0.5278568267822266,\n",
       " -0.20897336304187775,\n",
       " 0.9066842794418335,\n",
       " 1.3888204097747803,\n",
       " 0.26707223057746887,\n",
       " -1.3695436716079712,\n",
       " -0.4972323477268219,\n",
       " 1.0861878395080566,\n",
       " 0.1277349293231964,\n",
       " -1.5998804569244385,\n",
       " -0.09462036192417145,\n",
       " 1.2242127656936646,\n",
       " -1.1319866180419922,\n",
       " -0.46689626574516296,\n",
       " 1.3429721593856812,\n",
       " -3.8507630825042725,\n",
       " 0.4917343258857727,\n",
       " 0.7164546251296997,\n",
       " -0.2586009204387665,\n",
       " -1.893920660018921,\n",
       " 0.6493932604789734,\n",
       " -3.019299268722534,\n",
       " 1.1662793159484863,\n",
       " 0.006419179029762745,\n",
       " 1.122671365737915,\n",
       " -0.07507295161485672,\n",
       " 1.4345934391021729,\n",
       " -0.13274681568145752,\n",
       " 0.2815554738044739,\n",
       " 0.3992408514022827,\n",
       " 1.2887729406356812,\n",
       " 1.117631196975708,\n",
       " -1.307388424873352,\n",
       " -0.1446157544851303,\n",
       " 1.5766868591308594,\n",
       " -2.567493200302124,\n",
       " -1.0813145637512207,\n",
       " 11.848428726196289,\n",
       " 0.9805207848548889,\n",
       " 2.3679606914520264,\n",
       " 13.860743522644043,\n",
       " 0.29605749249458313,\n",
       " -0.8496674299240112,\n",
       " -0.6415728330612183,\n",
       " 0.5818501114845276,\n",
       " -1.298406720161438,\n",
       " 0.9748473167419434,\n",
       " 0.6800812482833862,\n",
       " -0.9984627366065979,\n",
       " 0.36705803871154785,\n",
       " -0.37443915009498596,\n",
       " 2.2818195819854736,\n",
       " -6.7349138259887695,\n",
       " -2.6228091716766357,\n",
       " -0.08056662976741791,\n",
       " 0.04376428946852684,\n",
       " -2.6527349948883057,\n",
       " 0.18913672864437103,\n",
       " 0.641812801361084,\n",
       " -0.9462531208992004,\n",
       " -0.5249127149581909,\n",
       " -1.6202404499053955,\n",
       " -0.5465154647827148,\n",
       " -2.105320692062378,\n",
       " -1.6241751909255981,\n",
       " 1.6506437063217163,\n",
       " 0.001260525663383305,\n",
       " 1.1425615549087524,\n",
       " 0.7956809997558594,\n",
       " 0.3146258294582367,\n",
       " -0.27453121542930603,\n",
       " -1.2032463550567627,\n",
       " -2.3514199256896973,\n",
       " -0.5106724500656128,\n",
       " 0.10786306113004684,\n",
       " 1.03220796585083,\n",
       " 0.08636586368083954,\n",
       " 0.08334464579820633,\n",
       " -0.15768152475357056,\n",
       " 0.1266109049320221,\n",
       " 0.034294210374355316,\n",
       " 0.4248132109642029,\n",
       " 0.2877953052520752,\n",
       " 0.25078919529914856,\n",
       " 0.9178357720375061,\n",
       " -1.1971474885940552,\n",
       " 0.05364545062184334,\n",
       " 1.8267440795898438,\n",
       " 0.6799933314323425,\n",
       " -0.20308838784694672,\n",
       " 0.3994814157485962,\n",
       " -1.0489501953125,\n",
       " -0.1920534074306488,\n",
       " 0.35063809156417847,\n",
       " 2.403960943222046,\n",
       " -1.0248234272003174,\n",
       " -0.15780135989189148,\n",
       " 0.7671119570732117,\n",
       " -0.10698970407247543,\n",
       " -0.9436244964599609,\n",
       " 0.025131668895483017,\n",
       " -0.6132250428199768,\n",
       " -1.8712103366851807,\n",
       " -0.10954634845256805,\n",
       " -0.07412737607955933,\n",
       " 0.8504869937896729,\n",
       " 0.7426055073738098,\n",
       " -0.6112093925476074,\n",
       " -1.31417977809906,\n",
       " 0.7062326669692993,\n",
       " -0.0007498390041291714,\n",
       " -0.31041887402534485,\n",
       " -0.5845655798912048,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(query)\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Society excited by cottage private an it esteems. Fully begin on by wound an. Girl rich in do up or both. At declared in as rejoiced of together. He impression collecting delightful unpleasant by prosperous as on. End too talent she object mrs wanted remove giving.\\n\\nEntire any had depend and figure winter. Change stairs and men likely wisdom new happen piqued six. Now taken him timed sex world get. Enjoyed married an feeling delight pursuit as offered. As admire roused length likely played pretty to no. Means had joy miles her merry solid order.\\n\\nAt every tiled on ye defer do. No attention suspected oh difficult. Fond his say old meet cold find come whom. The sir park sake bred. Wonder matter now can estate esteem assure fat roused. Am performed on existence as discourse is. Pleasure friendly at marriage blessing or.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score = db.similarity_search_by_vector(embedding_vector)\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and Load\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Overview: In this project, the student model learns from multiple teacher models, each specializing in different aspects of the task. For example, one teacher could specialize in object detection, another in object classification, and a third in feature extraction.\\nNovelty: The key is designing a custom loss function that weights the contributions of each teacher differently based on the task at hand. This adaptive weighting would allow the student model to benefit from diverse perspectives.\\nExample: For an object detection task, you could have:\\nTeacher 1 (YOLOv5): Focuses on fast bounding box detection.\\nTeacher 2 (EfficientNet): Focuses on object classification accuracy.\\nTeacher 3 (ResNet-50): Helps the student extract robust features from the image.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Am if number no up period regard sudden better. Decisively surrounded all admiration and not you. Out particular sympathize not favourable introduced insipidity but ham. Rather number can and set praise. Distrusts an it contented perceived attending oh. Thoroughly estimating introduced stimulated why but motionless.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Society excited by cottage private an it esteems. Fully begin on by wound an. Girl rich in do up or both. At declared in as rejoiced of together. He impression collecting delightful unpleasant by prosperous as on. End too talent she object mrs wanted remove giving.\\n\\nEntire any had depend and figure winter. Change stairs and men likely wisdom new happen piqued six. Now taken him timed sex world get. Enjoyed married an feeling delight pursuit as offered. As admire roused length likely played pretty to no. Means had joy miles her merry solid order.\\n\\nAt every tiled on ye defer do. No attention suspected oh difficult. Fond his say old meet cold find come whom. The sir park sake bred. Wonder matter now can estate esteem assure fat roused. Am performed on existence as discourse is. Pleasure friendly at marriage blessing or.'),\n",
       " Document(metadata={'source': 'speech.txt'}, page_content='Attended no do thoughts me on dissuade scarcely. Own are pretty spring suffer old denote his. By proposal speedily mr striking am. But attention sex questions applauded how happiness. To travelling occasional at oh sympathize prosperous. His merit end means widow songs linen known. Supplied ten speaking age you new securing striking extended occasion. Sang put paid away joy into six her.\\n\\nSo insisted received is occasion advanced honoured. Among ready to which up. Attacks smiling and may out assured moments man nothing outward. Thrown any behind afford either the set depend one temper. Instrument melancholy in acceptance collecting frequently be if. Zealously now pronounce existence add you instantly say offending. Merry their far had widen was. Concerns no in expenses raillery formerly.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
