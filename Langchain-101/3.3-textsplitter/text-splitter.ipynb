{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='arXiv:1812.00660v1  [cs.LG]  3 Dec 2018Knowledge Distillation with Feature Maps for\\nImage Classiﬁcation\\nWei-Chun Chen, Chia-Che Chang, Chien-Yu Lu, and Che-Rung Lee\\nNational Tsing Hua University, Taiwan\\n{meatybobby,chang810249,j19550713 }@gmail.com,\\ncherung@cs.nthu.edu.tw\\nAbstract. The model reduction problem that eases the computation\\ncosts and latency of complex deep learning architectures ha s received an\\nincreasing number of investigations owing to its importanc e in model de-\\nployment. One promising method is knowledge distillation ( KD), which\\ncreates afast-to-execute studentmodel to mimic alarge tea cher network.\\nIn this paper, we propose a method, called KDFM (Knowledge Di stil-\\nlation with Feature Maps), which improves the eﬀectiveness of KD by\\nlearning the feature maps from the teacher network. Two majo r tech-\\nniques used in KDFM are shared classiﬁer and generative adve rsarial\\nnetwork. Experimental results show that KDFM can use a four l ayers\\nCNN to mimic DenseNet-40 and use MobileNet to mimic DenseNet -100.\\nBoth student networks have less than 1% accuracy loss compar ing to\\ntheir teacher models for CIFAR-100 datasets. The student ne tworks are\\n2-6 times faster than their teacher models for inference, an d the model\\nsize of MobileNet is less than half of DenseNet-100’s.\\nKeywords: Knowledge Distillation ·Model Compression ·Generative\\nAdversarial Network.\\n1 Introduction\\nDeep learning has shown its capability of solving various computer visio n prob-\\nlems, such as image classiﬁcation [18] and object detection [5]. Its su ccess also\\nenables many related applications, such as self-driving cars[2], medic al diagnosis\\n[22], and intelligent manufacturing [32].\\nHowever,thestate-of-the-artdeeplearningmodelsusuallyhave largememory\\nfootprintsandrequireintensivecomputationalpower.Forinstan ce,VGGNet [31]\\nrequires more than 100 million parameters and more than 15 giga ﬂoat ing-point-\\noperations (GFLOPs) to inference an image of 224 ×224resolution. It is diﬃcult\\nto deploy these models on some platforms with limited resources, suc h as mobile\\ndevices, or Internet of Things (IOT) devices. In addition, the infe rence time may\\nbe too long to satisfy the real-time requests of tasks.\\nMany methods have been proposed to reduce the computational c osts of\\ndeep learning models during the inference time. For instance, the we ight quan-\\ntization method [14] reduces the network size by quantizing the net work param-\\neters. Structure pruning [7] is another example that removes the unnecessary'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='2 W. Chen et al.\\nparameters or channels of a trained convolutional neural networ k (CNN), and\\nthen ﬁne-tunes the model to gain higher accuracy. These method s have achieved\\ncompetitive accuracy with less model size comparing to those of orig inal models.\\nAlthough they can eﬀectively reduce the model sizes and inference time, their\\noperations are usually not matching the instructions of commodity a cceleration\\nhardware, such as GPU or TPU. As a result, the real performance gain of those\\nmethods may not be signiﬁcant comparing to the original models with h ardware\\nacceleration.\\nAnother promising directions of model reduction is Knowledge Distillat ion\\n(KD) [10], whose idea is to train a student network to mimic the ability of a\\nteachermodel.Thestudentmodelisusuallysmallerorfaster-to-e xecutethanthe\\nteacher model. Hinton & Dean [10] coined the name of Knowledge Distilla tion\\n(KD). They trained student networks by the “soft target”, a mo dify softmax\\nfunction which can provide more information than the traditional so ftmax func-\\ntion. The experiment shows KD can improve the performance of a sin gle shallow\\nnetwork by distilling the knowledge in an ensemble model. Romero & Beng io\\n[27] extended the idea of KD and proposed FITNET. They trained th inner and\\ndeeper student networks by the “intermediate-level hint”, which is from the hid-\\nden layers of the teacher network, and the “soft target” to lear n the teacher\\nnetwork. The results show that FITNET can use fewer parameter s to mimic the\\nteachernetwork.In [34], Xu &Huang proposedthe method that us es conditional\\nadversarial networks to make student networks learn the logits o f the teacher\\nnetworks.Their experimentsshowedthat it canfurther improvet he performance\\nof student models trained by traditional KD.\\nHowever,those KD methods only learn the logits ofteachermodels. They are\\nusually not powerful enough to make student models mimic all kinds of teacher\\nmodels well. They often need to customize the student models for sp eciﬁc archi-\\ntectures. In addition, as the deep models become more and more co mplicated,\\nthe eﬀectiveness of previous methods for knowledge distillation dec reases. One\\nexample is DenseNet [12], which connects all layers directly with each o ther,\\nand requires more computation in inference time. In our experiment s, the simple\\nCNN student models learned from previous methods cannot achieve the similar\\naccuracy as the teacher model.\\nIn this paper, we propose a method, called KDFM (Knowledge Distillatio n\\nwith Feature Maps), which learns the feature maps from the teach er model. For\\nthe application of image classiﬁcation, feature maps often provide m ore informa-\\ntion than logits. The feature maps in the last layer are used because they possess\\nthe high level features of the input images, which are the most infor mative for\\nclassiﬁcation. KDFM utilizes two techniques to distill the knowledge of feature\\nmaps. First, it lets the teacher model and the student model shar e the classiﬁer.\\nThrough the training of the shared classiﬁer, the student model c an learn the\\nfeature maps from the teacher. Second, the idea of generative a dversarial net-\\nworks (GANs) [6] is used to improve the learning process. The featu re map in\\nCNN is a special type of images. During the learning process, the disc rimina-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='KDFM 3\\ntor is forcing the student model (generator) to generate similar f eature maps to\\nthose of the teacher model (inputs of GANs).\\nAlthough the method could be generally applied to other types of net works,\\nwe employ the DenseNets as the teacher models to illustrate the idea and to\\ndemonstrate its eﬀectiveness in the experiments. Unlike FITNET [2 7] whose\\nstudent models are thin and deep, we let the student models be shallo w and fat,\\nbecause such kind of networks are easier to be parallelized on moder n accelera-\\ntors, such as GPU.\\nWe validated the eﬀectiveness of KDFM using CIFAR-100 datasets [1 7]\\nand ImageNet datasets [4]. The ﬁrst experiment uses a simple stude nt network\\nwhich only contains 4 convolutional layers and a fully-connected laye r to mimic\\nDenseNet-40(DenseNet with 40layers)onCIFAR-100.The result showsthe stu-\\ndentmodelgeneratedbyKDFMhaslessthan1%accuracylossand2 timesfaster\\ninference time comparing to DenseNet-40, which is better than oth er methods.\\nThe second experiment trains the model of MobileNet [11], a state-o f-the-art\\nnetwork for mobile and embedded platforms, to mimic DenseNet-100 (DenseNet\\nwith 100 layers) on CIFAR-100. The results show that the student model is\\nmore than 6 times faster than DenseNet-100 in terms of inference time, with\\nonly half model size and less than 1% accuracy loss. The third experim ent uses\\nMobileNet v2 [29] to mimic ResNet-152 [9] on ImageNet, and the accur acy of\\nKDFM is better than other KD methods.\\nThe rest of paper is organized as follows. Section 2 gives a brief illustr ation of\\nknowledge distillation (KD) and generative adversarial networks (G ANs). Sec-\\ntion 3 introduces the design of KDFM to construct a student model. Section\\n4 shows the experimental results and the performance compariso n with other\\nmethods. The conclusion and future work are presented in the last section.\\n2 Related Work\\n2.1 Knowledge Distillation\\nIn [1], Ba& Carunaaskedan interestingquestion,“DoDeep Nets Rea llyNeed to\\nbe Deep?” Their answer is that shallow nets can be trained to perfor m similarly\\ntocomplex,well-engineered,deeperconvolutionalmodels.Themet hod theyused\\nto train shallow networks is mimicking the teacher networks’ logits, t he value\\nbefore the softmax activation. In 2017, authors presented mor e experimental\\nresults in [33].\\nHinton & Dean [10] generalized this idea as Knowledge Distillation (KD).\\nThe concept of knowledge distillation is to train a student network by a hard\\ntargetPHand a soft target PS:\\nPH(x) =softmax (x) (1)\\nPS(x,t) =softmax(x\\nt)\\n(2)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='4 W. Chen et al.\\nwherexare logits in a neural network, and tis a hyper-parameter, t >1, to\\nsoften the probability distribution over classes. A higher value of tcould provide\\nmore information.\\nLetxTbe the logitsofthe teachernetworkand xSbe the logitsofthe student\\nnetwork. The goal of student network is to optimize the loss funct ion\\nLKD=λLH+(1−λ)LS, (3)\\nwhere\\nLH=H(PH(xS),y)) and (4)\\nLS=H(PS(xS,t),PS(xT,t)) (5)\\nandyisground-truthlabel.Theytrainedshallownetworksbythe“softt arget”of\\nteachernetworks.KDsoftens the output ofthe softmax funct ion, providingmore\\ninformation than traditional softmax functions. The experiment in this paper\\nshows KD can improve the performance of a model by distilling the kno wledge\\nin an ensemble model into a single model.\\nRomero & Bengio [27] proposed FITNET, which extends the idea of KD by\\nusing “intermediate-level hints” from the hidden layersof the teac her network to\\nguide the student networks. They train thinner and deeper stude nt networks to\\nlearnthe intermediaterepresentationsand the soft targetofth e teachernetwork.\\nThe results show that the student network of FITNET can perfor m comparable\\nor even better than the teacher network with fewer parameters .\\n2.2 Generative Adversarial Networks\\nGenerative Adversarial Networks (GANs) have shown impressive r esults for un-\\nsupervised learning tasks, such as image generation [6], image synth esis [26], and\\nimage super-resolution [19]. A GAN usually consists of two modules: a g ener-\\nator (G) and a discriminator (D). In a typical GAN model, the discrimin ator\\nlearns to distinguish real samples and fake results produced by the generator,\\nand the generator learns to create samples which can be judged as real ones by\\nthe discriminator.\\nMirza & Osindero [21] extended GANs to a conditional model by feedin g\\nextra information, such as class labels, to the generator and discr iminator. Chen\\n& Abbeel [3] proposed InfoGAN, an information-theoretic extens ion to GANs,\\nwhich is able to learn disentangle representation. Some studies [24,2 8,25] modify\\nthe discriminator to contain an auxiliary decoder network that can o utput class\\nlabels for training data.\\n2.3 DenseNet\\nHuang&Weinberger[12]proposedanewarchitecture,DenseNet ,whichconnects\\nall layers directly with each other. This idea is extended from ResNet [8] which\\naggregatespreviousfeature maps and feeds the summation into a layer.Diﬀerent'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='KDFM 5\\nfrom ResNet, DenseNet concatenates the feature maps from all preceding layers.\\nIt requires fewer parameters than traditional convolutional net works, because\\nit doesn’t need to relearn redundant feature maps. It performs s tate-of-the-art\\nresults on most classiﬁcation benchmark tasks.\\n2.4 MobileNet\\nHoward& Kalenichenko[11] proposedMobileNet for mobile and embedd ed plat-\\nforms. MobileNet uses depth-wise separable convolutions to reduc e the compu-\\ntation and build a light-weight network. MobileNet allows to build the mod el\\non resource and accuracy trade-oﬀs by using width multiplier and re solution\\nmultiplier. The eﬀectiveness of MobileNet has been demonstrated ac ross a wide\\nrange of applications.\\n3 The Design of KDFM\\nKDFMuseaGANwithanauxiliarydecodernetworkthatcanoutputcla sslabels\\nfor training data. More speciﬁcally, it consists of three component s, a generator\\nG, a discriminator D, and a classiﬁer C. The generator Gis a feature extractor\\nwho produces the feature maps from the input images. The discrimin atorD\\ndistinguishes the real feature map, generated by the teacher ne twork, and the\\nfake feature map, generated by G. The classiﬁer Cis a feature decoder, whose\\ninputs arealso feature maps, and outputs are the hard target an d the soft target,\\nas deﬁned in (1) and (2).\\nThe goal of KDFM is to make Glearn the feature map from the teacher\\nnetwork, and to train Cto classify the images based on the feature maps. Two\\nobjective functions, adversarialloss and knowledge distillation loss , are designed\\nto achieve the goal. The adversarial loss of KDFM is adopted from th e objective\\nfunction of LSGAN [20],\\nLadvD=1\\n2[D(G(X))]2+1\\n2[D(T(X))−1]2(6)\\nLadvG=1\\n2[D(G(X))−1]2(7)\\nwhereXdenotes the input images, G(X) is the feature maps generated by G,\\nandT(X) is the feature maps generated by the teacher model. The functio nDis\\ndesigned to discriminate between the realfeature map T(X)and the fake feature\\nmapG(X). We chose LSGAN because it is the state-of-the-art GAN model\\nand the range of its loss function can be easily combined with the know ledge\\ndistillation loss.\\nThe knowledge distillation loss in KDFM is deﬁned as below:\\nLKD=λLH+(1−λ)LS (8)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='6 W. Chen et al.\\nwhere\\nLH=H(PH(C(G(X))),PH(z))+H(PH(C(T(X))),PH(z)) (9)\\nLS=H(PS(C(G(X)),t),PS(z,t))+H(PS(C(T(X)),t),PS(z,t)) (10)\\nthe value zis the logits from the teacher network, Hrefers to cross-entropy,and\\nλis a hyper-parameter, 0 < λ <1, controlling the ratio of LHandLS. If the\\nstudent model is similar to the teacher model, λneed not be large. Besides, we\\nchange the ground-truth label to the label of the teacher netwo rk in (9). The\\nexperiment also shows that it achieves better accuracy.\\nUnlike traditional GAN, the loss function of Gin KDFM combines the ad-\\nversarial loss and the knowledge distillation loss,\\nLG=LadvG+αLKD (11)\\nwhereαis a hyper-parameter to balance the scale of the adversarial loss a nd the\\nknowledge distillation loss.\\nThe training of KDFM is to minimize the loss functions of three compone nts\\nsimultaneously. For the generator G, the loss function is LG, as deﬁned in (11);\\nfor the discriminator D, the loss function is LadvD, as deﬁned in (6); and for the\\nclassiﬁer C, the loss function is LKD, deﬁned in (8).\\nInput  \\nimageTeacher  \\nnetwork\\nGReal\\nfeature  map\\nFake\\nfeature  mapD\\nCReal  /  Fake\\nHard  targetSoft  target\\nStudent  network\\nFig.1.Overview of KDFM, consisting of three module, a discriminat or D, a generator\\nG, andaclassiﬁer C. G andCcompose astudentnetwork. Thestu dentnetwork outputs\\nthe hard target for the inference.\\nFigure 1 shows the network architecture of KDFM. The student ne twork\\nconsists of two parts, the feature extractor Gand the feature decoder C. The\\nfeature extractor generates the feature map, and the featur e decoder classiﬁes\\nthe feature map to probability distribution over classes. After eac h components\\narewell-trained,the student networkis constructedfrom GandC. In ourdesign,\\nConly has a pooling layer and one fully connected layer.\\nThe training process works like the alternative least square (ALS) m ethod.\\nLet’s use LHto illustrate the idea, since LShas the same structure. To minimize\\nH(PH(C(T(X))),PH(z)), the classiﬁer Cneeds to learn teacher network’s hard'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='KDFM 7\\ntargetPH(z). Meanwhile, the term LKDis also added to the loss function of the\\nstudent network. To minimize the H(PH(C(G(X))),PH(z)), the student model\\nmust output feature maps G(X) similar to T(X), so that PH(C(G(X))) can\\napproximate PH(z).\\n4 Experiments\\nWevalidatedtheeﬀectivenessofKDFMusingCIFAR-100andImageN etdatasets.\\nWe used DenseNet and ResNet as the teacher models, whose impleme ntations\\n[16] are in TensorFlow, and followed the standard training process w ith data\\naugmentation. Two types of student models are used in the experim ents. The\\nﬁrst kind of student models are simple convolutional neural networ ks (CNNs)\\nthat consist of several convolutional layers and one fully-connec ted layer, with\\nReLUactivation[23],batchnormalization[15],andmax-poolinglayers. Thecon-\\nvolutional layers are with 3 ×3 kernel size, and 64 to 1024 channels, depending\\non the parameter sizes. The second student model is MobileNet, wh ich has a\\nTensorﬂow implementation [30] on Github. We modiﬁed the student mo dels so\\nthat the dimension ofstudent model’s feature maps equal to the te achermodel’s.\\nWithout further speciﬁcation, the hyper-parameter tandλ, as deﬁned in (2) and\\n(8), are set to 10 and 0 .1 respectively. The hyper-parameters α, deﬁned in (11),\\nis set to 10. The performance metrics of models are the accuracy a nd the infer-\\nence time, which is obtained from the average inference time of pred icting one\\nCIFAR-100 image 1000 times on one NVIDIA 1080Ti GPU.\\n4.1 Teacher Network: DenseNet-40\\nThis set of experiments uses various CNN models to mimic DenseNet-4 0. We\\ncompare the results of KDFM with other knowledge distillation method s, and\\njustify the inﬂuence of four factors to the accuracy and the infe rence time: the\\nnumber of layers, the number of parameters, the value of hyper- parameter tand\\nλ, deﬁned in (2) and (8).\\nComparison with other methods. We compared the accuracyof the student\\nnetwork generated by KDFM and other two knowledge distillation met hods:\\nlogits mimic learning [1] and KD [10]. We also included the results of the mod el\\ntrained without any KD process as the baseline. The teacher model is DenseNet-\\n40 and the student model has 8 convolutional layers and 8 million train able\\nparameters. Table 1 shows the results of diﬀerent training method s. The result\\nindicates that the student model trained by KDFM can acheive similar accuracy\\nas the teacher model’s. Logits mimic learning performs poorly. Its ac curacy is\\neven lower than that of the baseline in this case.\\nDiﬀerent number of layers. Table 2 summarizes setting of student and\\nteacher models, and their experimental results. There are four s tudent mod-\\nels which have 2, 4, 6, 8 convolutional layers respectively. We ﬁxed t he number'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='8 W. Chen et al.\\nMethod Accuracy\\nBaseline 68.53%\\nLogits Mimic Learning 50.95%\\nKD 69.14%\\nKDFM 74.10%\\nTeacher(DenseNet-40) 74.23%\\nTable 1. Testingaccuracy for training thestudentnetworks with 8co nvolutional layers\\nand 8M parameters by Baseline (typical training process), L ogits Mimic Learning, KD,\\nand KDFM.\\nof parameters to 8 millions. As can be seen, when the number of conv olution\\nlayersis largerthan4, the student models achievesimilaraccuracya sthe teacher\\nmodel. Although the model size is not small, the inference time of stud ent mod-\\nels is much shorter than that of the teach network. Particularly, t he student\\nnetwork with 4 convolution layers has better accuracy than the te acher model,\\nand its inference time is only half of the teacher model’s.\\nFigure2plots theaccuracyofstudent networksfordiﬀerentnum ber oflayers.\\nA clear trend is that when the number of layers is larger than 4, the s tudent\\nmodel can achieve similar accuracy as the teacher model. However, when the\\nnumber of layers is small, even with a large number of parameters, th e student\\nmodel cannotlearn well as the teacher model. This result matches t he conclusion\\nmade in [33].\\nModel No. Parameters Accuracy Inference time\\n2 conv ∼8M 59.19% 3.65ms\\n4 conv ∼8M 74.77% 2.46ms\\n6 conv ∼8M 74.08% 2.59ms\\n8 conv ∼8M 74.10% 2.75ms\\nDenseNet-40(Teacher) 1.1M 74.23% 5.28ms\\nTable 2. Testing accuracy and inference time for the student network s with 2, 4, 6,\\nand 8 convolutional layers mimicking DenseNet-40 by KDFM.\\nDiﬀerent number of parameters. Table 3 lists the setting and the results of\\nsix student models with diﬀerent number of parameters. The numbe r of layers\\nof CNNs is ﬁxed at 4, and the number of parameters are varied from 0.5M, 1M,\\n2M, 4M, 6M, to 8M. As can be seen, the more parameters, the bett er accuracy\\nof the model. When the number of parameters is larger than or equa l to 4M, the\\naccuracy of student model is similar to that of the teacher model. T he diﬀerence\\nis less than 1%. Figure 3 shows this trend.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 8}, page_content='KDFM 9\\n2 3 4 5 6 7 8\\n# of conlolutional layers6062646668707274Accuracy(%)student\\nteacher\\nFig.2.Accuracy of student networks with diﬀerent convolution lay ers and 8 million\\nparameters, the horizontal line is the accuracy of the teach er network, DenseNet-40.\\nHowever,theinferencetimeofstudentmodelsisalsoincreasingast henumber\\nof parameter increases. Nevertheless, even when the number of parameter is 8M,\\nthe inference time is still less than half of the teacher model’s. The tr ade-oﬀ\\nbetween accuracy and the inference time can be used to adjust th e student\\nmodels to ﬁt the requirements of deployments.\\nModel No. Parameters Accuracy Inference time\\n4conv-0.5M ∼0.5M 65.76% 1.61ms\\n4conv-1M ∼1M 67.83% 1.65ms\\n4conv-2M ∼2M 71.12% 1.73ms\\n4conv-4M ∼4M 73.77% 2.01ms\\n4conv-6M ∼6M 73.84% 2.32ms\\n4conv-8M ∼8M 74.77% 2.46ms\\nDenseNet-40(Teacher) 1.1M 74.23% 5.28ms\\nTable 3. Testing accuracy and inference time for the student network s with 4 convolu-\\ntional layers and diﬀerent numbers of parameters mimicking DenseNet-40 by KDFM.\\nDiﬀerent hyper-parameter t.We validated the inﬂuence of hyper-parameter\\nt, deﬁned in (2), to the accuracy of student models. Table 4 and Tab le 5 show\\nthe results for two models, one is a 4 layer CNNs with 2M parameters ( small\\nmodel), and the other is a 4 layer CNNs with 8M parameters (large mod el). As'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 9}, page_content='10 W. Chen et al.\\n12345678\\n# of trainable parameters(M)6668707274Accuracy(%)student\\nteacher\\nFig.3.Accuracy of student networks with 4 convolution layers and d iﬀerent number\\nof parameters, the horizontal line is the accuracy of the tea cher network, DenseNet-40.\\ncan be seen, the best result occurs at t= 5 for the small model and at t= 10 for\\nthe large model. This phenomenon can be reasoned as follows. When tis small,\\nthe soft target does not have enough relaxation to encourage st udent networks\\nlearning the teacher model. On the other hand, when tis too large, the teacher\\nmodel losses the disciplines to coach the student models. For weake r models,\\nsmallertcan usually give better accuracy, because they need clearer guide lines\\nto learn.\\nModel tAccuracy\\n4conv with t= 2 2 70.14%\\n4conv with t= 5 5 71.48%\\n4conv with t= 10 10 71.12%\\n4conv with t= 50 50 67.35%\\n4conv with t= 100 100 67.51%\\nDenseNet-40(Teacher) - 74.23%\\nTable 4. Testing accuracy for the student networks with 4 convolutio nal layers, 2M\\nparameters, and diﬀerent hyper-paramter tmimicking DenseNet-40 by KDFM.\\nDiﬀerent hyper-parameter λ.This experiment compares the accuracy of\\nstudent models for diﬀerent hyper-parameter λ, deﬁned in (8). Table 11 and\\nTable 7 show the results for two models, one is a 4 layer CNNs with 2M pa ram-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 10}, page_content='KDFM 11\\nModel tAccuracy\\n4conv with t= 2 2 73.11%\\n4conv with t= 5 5 74.07%\\n4conv with t= 10 10 74.77%\\n4conv with t= 50 50 71.44%\\n4conv with t= 100 100 70.72%\\nDenseNet-40(Teacher) - 74.23%\\nTable 5. Testing accuracy for the student networks with 4 convolutio nal layers, 8M\\nparameters, and diﬀerent hyper-paramter tmimicking DenseNet-40 by KDFM.\\neters (small model), and the other is a 4 layer CNNs with 8M paramete rs (large\\nmodel). For both models, the best result occurs at λ= 0.1.\\nThe results indicate the importance of soft target in knowledge dist illation.\\nForλ= 0.1, the value of soft target dominates the loss function of KD. This\\nshows that with more information, student models can learn better . However, if\\nλis set to 0, the information of hard target totally disappears, and t he student\\nmodel cannot learn the best results from the teacher model.\\nModel λAccuracy\\n4conv with λ= 0 0 70.87%\\n4conv with λ= 0.1 0.1 71.12%\\n4conv with λ= 0.4 0.4 67.30%\\n4conv with λ= 0.7 0.7 66.96%\\nDenseNet-40(Teacher) - 74.23%\\nTable 6. Testing accuracy for the student networks with 4 convolutio nal layers, 2M\\nparameters, and diﬀerent hyper-parameter λmimicking DenseNet-40 by KDFM.\\nModel λAccuracy\\n4conv with λ= 0 0 74.11%\\n4conv with λ= 0.1 0.1 74.77%\\n4conv with λ= 0.4 0.4 73.18%\\n4conv with λ= 0.7 0.7 71.68%\\nDenseNet-40(Teacher) - 74.23%\\nTable 7. Testing accuracy for the student networks with 4 convolutio nal layers, 8M\\nparameters, and diﬀerent hyper-parameter λmimicking DenseNet-40 by KDFM.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='12 W. Chen et al.\\nDiﬀerent hyper-parameter α.The hyper-parameter αcontrols the ratio of\\nGAN and KD in generator’s loss function, LG=LadvG+αLKD. Table 8 lists\\nthe achieved accuracy of student model for diﬀerent α. The best result occurs\\natα= 10 in our experiments. If we take oﬀ GAN LadvG, as shown in the third\\nline, the accuracy also declines.\\nModel αAccuracy\\n4conv with α= 1 1 69.58%\\n4conv with α= 10 10 70.62%\\n4conv without LadvGLG=LKD69.57%\\nDenseNet-40(Teacher) - 74.23%\\nTable 8. Testing accuracy for the student networks with 4 convolutio nal layers, 6M\\nparameters, and diﬀerent hyper-parameter αmimicking DenseNet-40 by KDFM.\\n4.2 Teacher Network: DenseNet-100\\nSinceoneofthe goalsforknowledgedistillationistocreatemodelseas ytodeploy\\non small devices, in this experiment, we used MobileNet (student net work) to\\nmimic DenseNet-100 (teacher network). For comparison, we includ ed the results\\nof two other CNNs trained by KDFM. Both CNNs have 8 convolutional layers,\\nand one has 20.2M parameters; the other has 28.1M parameters. I n addition,\\nthe result of MobileNet, trained directly without KDFM, is also included as the\\nbaseline.\\nTable 9 summarizes the results. The ﬁrst three rows are the netwo rks trained\\nby KDFM, the fourth row is the result for MobileNet without KD, and t he last\\nrow is the result of DenseNet-100. As shown in the ﬁrst two rows, s imple CNNs,\\neven with large amount of parameters, cannot achieve good accur acy as the\\nteacher model. But their inference times (4.56ms and 5.7ms) are muc h shorter\\nthan that of the original DenseNet-100 (18.02ms).\\nThe MobileNet trained by KDFM, as shown in the third row, has the bes t\\nresult in terms of model size, accuracy, and inference time. The nu mber of pa-\\nrameters of MobileNet (3.5M) is less than half of DenseNet-100’s (7.2 M), and\\nthe inference time (2.79ms) is about 6 times faster than the original DenseNet-\\n100 (18.02ms). Comparing to the baseline, MobileNet without KD, the Mo-\\nbileNet trained by KDFM can achieve 77.20% accuracy, which is close to that\\nof DenseNet-100 (77.94%).\\n4.3 Teacher Network: CondenseNet\\nWe use CondenseNet-86 [13] (with stages [14, 14, 14] and growth [8 , 16, 32]) as\\nthe teacher network and a smaller CondenseNet-86 (with stages [1 4, 14, 14] and\\ngrowth [8, 16, 16]) as the student network using CIFAR-100 datas et. The results\\nare shown in Table 10. The model trained by KDFM is improved.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 12}, page_content='KDFM 13\\nModel No.Parameters Accuracy Inference time\\n8 conv-20M (KDFM) 20.2M 74.36% 4.56ms\\n8 conv-28M (KDFM) 28.1M 75.25% 5.7ms\\nMobileNet (KDFM) 3.5M 77.20% 2.79ms\\nMobileNet(Baseline) 3.5M 72.99% 2.79ms\\nDenseNet-100(Teacher) 7.2M 77.94% 18.02ms\\nTable 9. Testing accuracy and inference time for training simple CNN s with 8 convo-\\nlutional layers and 20.2M, 28.1M parameters, and MobileNet as student networks by\\nKDFM.\\nModel No. Parameters FLOPs Accuracy\\nSmaller CondenseNet-86 (Baseline) 0.29M 49.95M 74.13%\\nSmaller CondenseNet-86 (KDFM) 0.29M 49.95M 75.01%\\nCondenseNet-86(Teacher) 0.55M 65.85M 76.02%\\nTable 10. Testing accuracy for the smaller CondenseNet-86 mimicking CondenseNet-\\n86 by KDFM and Baseline (typical training process) on CIFAR- 100 dataset.\\n4.4 ImageNet dataset\\nWe used MobileNet v2 as the student model to mimic the pre-trained R esNet-\\n152 using ImageNet dataset. Table 11 shows the experimental res ults of the\\ntesting accuracy using diﬀerent training methods for the student model. As can\\nbe seen, the baseline method (without KD) can only achieve 68.41% ac curacy.\\nThe KDFM model has the best result, 71.82%.\\nModel Accuracy Inference time FLOPs\\nMobileNet v2(KDFM) 71.82% 6ms 300M\\nMobileNet v2(KD) 70.16% 6ms 300M\\nMobileNet v2(KDFM without LadvG&LG=LKD) 71.32% 6ms 300M\\nMobileNet v2(Baseline) 68.01% 6ms 300M\\nResNet-152(Pre-trained teacher) 78.31% 21ms 11G\\nTable 11. Testing accuracy for MobileNet v2 as the student networks, t rained by\\nKDFM, KD, KDFM without LadvG, and MobileNet v2 (baseline), to mimic ResNet-\\n152 by KDFM.\\n5 Conclusion and Future Work\\nWe presented a novel architecture, KDFM, which utilizes generativ e adversarial\\nnetworks to achieve knowledge distillation. The experiments demons trate that'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='14 W. Chen et al.\\nKDFM can use simple convolutional neural networks with shallower lay ers and\\nlarger number of trainable parameters to mimic state-of-the-art complicated\\nnetworks with comparable accuracy and faster inference time.\\nThe ideaofusing generativeadversarialnetworksfor knowledged istillation is\\nnot limited to the DenseNet or image classiﬁcation tasks, but can be g eneralized\\nto other types of networks for diﬀerent applications. It is also ort hogonal to\\nother model compression methods, which means one can use KDFM t o generate\\na student model and apply model pruning or other compression tec hniques to\\nfurther reduce the model size and improve the performance. Las t, what is the\\nbest student models to be used in KDFM still requires more investigat ions. One\\ngood feature of KDFM is that other objectives, such as model size , inference\\nspeed, power consumption, ﬁtting speciﬁc hardware, can be incor porated into\\nthe student model design.\\nReferences\\n1. Ba, J., Caruana, R.: Do deep nets really need to be deep? In:\\nGhahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., We in-\\nberger, K.Q. (eds.) Advances in Neural Information Process -\\ning Systems 27, pp. 2654–2662. Curran Associates, Inc. (201 4),\\nhttp://papers.nips.cc/paper/5484-do-deep-nets-reall y-need-to-be-deep.pdf\\n2. Bojarski, M., Testa, D.D., Dworakowski, D., Firner, B., F lepp, B., Goyal, P.,\\nJackel, L.D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., Zieba,\\nK.: End to end learning for self-driving cars. CoRR abs/1604.07316 (2016),\\nhttp://arxiv.org/abs/1604.07316\\n3. Chen,X., Duan,Y., Houthooft, R.,Schulman,J., Sutskeve r,I., Abbeel,P.: Infogan:\\nInterpretable representation learning by information max imizing generative adver-\\nsarial nets. CoRR abs/1606.03657 (2016),http://arxiv.org/abs/1606.03657\\n4. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L .: ImageNet: A Large-\\nScale Hierarchical Image Database. In: CVPR09 (2009)\\n5. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-\\ncurateobject detection andsemantic segmentation. CoRR abs/1311.2524 (2013),\\nhttp://arxiv.org/abs/1311.2524\\n6. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., War de-Farley, D., Ozair, S.,\\nCourville, A.,Bengio, Y.:Generative adversarial nets. In :Ghahramani, Z., Welling,\\nM., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (eds.) Adv ances in Neural In-\\nformation Processing Systems 27, pp. 2672–2680. Curran Ass ociates, Inc. (2014),\\nhttp://papers.nips.cc/paper/5423-generative-adversa rial-nets.pdf\\n7. Hassibi, B., Stork, D.G.: Second order derivatives for ne twork pruning: Optimal\\nbrain surgeon. In: Hanson, S.J., Cowan, J.D., Giles, C.L. (e ds.) Advances in\\nNeural Information Processing Systems 5, pp. 164–171. Morg an-Kaufmann (1993),\\nhttp://papers.nips.cc/paper/647-second-order-deriva tives-for-network-pruning-optimal-brain-surgeon.pdf\\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: 2016 IEEE Conference on Computer Vision and Pattern Reco gnition (CVPR).\\npp. 770–778 (June 2016). https://doi.org/10.1109/CVPR.2 016.90\\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nCoRRabs/1512.03385 (2015),http://arxiv.org/abs/1512.03385'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='KDFM 15\\n10. Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowle dge in a Neural Network.\\nArXiv e-prints (Mar 2015)\\n11. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand,\\nT., Andreetto, M., Adam, H.: Mobilenets: Eﬃcient convoluti onal neural\\nnetworks for mobile vision applications. CoRR abs/1704.04861 (2017),\\nhttp://arxiv.org/abs/1704.04861\\n12. Huang, G., Liu, Z., v. d. Maaten, L., Weinberger, K.Q.: De nsely con-\\nnected convolutional networks. In: 2017 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR). pp. 2261–2269 (July 2017).\\nhttps://doi.org/10.1109/CVPR.2017.243\\n13. Huang, G., Liu, S., van der Maaten, L., Weinberger, K.Q.: Condensenet: An ef-\\nﬁcient densenet using learned group convolutions. In: The I EEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) (June 2018)\\n14. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., B engio, Y.: Quantized neu-\\nral networks: Training neural networks with low precision w eights and activations.\\nCoRRabs/1609.07061 (2016),http://arxiv.org/abs/1609.07061\\n15. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerati ng deep network train-\\ning by reducing internal covariate shift. CoRR abs/1502.03167 (2015),\\nhttp://arxiv.org/abs/1502.03167\\n16. Khlestov, I.: vision networks. https://github.com/ikhlestov/vision_networks\\n(2017)\\n17. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images\\n1(01 2009)\\n18. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet c lassiﬁcation with\\ndeep convolutional neural networks. In: Proceedings of the 25th Inter-\\nnational Conference on Neural Information Processing Syst ems - Vol-\\nume 1. pp. 1097–1105. NIPS’12, Curran Associates Inc., USA ( 2012),\\nhttp://dl.acm.org/citation.cfm?id=2999134.2999257\\n19. Ledig, C., Theis, L., Huszr, F., Caballero, J., Cunningh am, A., Acosta, A., Aitken,\\nA., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-\\nresolution using a generative adversarial network. In: 201 7 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR). pp. 105–11 4 (July 2017).\\nhttps://doi.org/10.1109/CVPR.2017.19\\n20. Mao, X., Li, Q., Xie, H., Lau, R.Y.K., Wang, Z., Smolley, S .P.: Least squares gen-\\nerative adversarial networks. In: 2017 IEEE International Conference on Computer\\nVision(ICCV).pp.2813–2821 (Oct2017). https://doi.org/ 10.1109/ICCV.2017.304\\n21. Mirza, M., Osindero, S.: Conditional generative advers arial nets. CoRR\\nabs/1411.1784 (2014),http://arxiv.org/abs/1411.1784\\n22. Mizotin, M., Benois-Pineau, J., Allard, M., Catheline, G.: Feature-based\\nbrain mri retrieval for alzheimer disease diagnosis. In: 20 12 19th IEEE In-\\nternational Conference on Image Processing. pp. 1241–1244 (Sept 2012).\\nhttps://doi.org/10.1109/ICIP.2012.6467091\\n23. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve re stricted boltzmann ma-\\nchines. In: Proceedings of the 27th International Conferen ce on International Con-\\nference on Machine Learning. pp. 807–814. ICML’10, Omnipre ss, USA (2010),\\nhttp://dl.acm.org/citation.cfm?id=3104322.3104425\\n24. Odena, A.: Semi-Supervised Learning with Generative Ad versarial Networks.\\nArXiv e-prints (Jun 2016)\\n25. Odena,A.,Olah,C.,Shlens,J.: Conditionalimage synth esiswithauxiliaryclassiﬁer\\nGANs. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34t h International Con-\\nference on Machine Learning. Proceedings of Machine Learni ng Research, vol. 70,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='16 W. Chen et al.\\npp. 2642–2651. PMLR, International Convention Centre, Syd ney, Australia (06–11\\nAug 2017), http://proceedings.mlr.press/v70/odena17a.html\\n26. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B. , Lee, H.: Generative ad-\\nversarial text to image synthesis. In: Balcan, M.F., Weinbe rger, K.Q. (eds.) Pro-\\nceedings of The 33rd International Conference on Machine Le arning. Proceedings\\nof Machine Learning Research, vol. 48, pp. 1060–1069. PMLR, New York, New\\nYork, USA (20–22 Jun 2016), http://proceedings.mlr.press/v48/reed16.html\\n27. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta , C., Bengio, Y.: Fitnets:\\nHints for thin deep nets. In: In Proceedings of ICLR (2015)\\n28. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., R adford, A., Chen,\\nX., Chen, X.: Improved techniques for training gans. In: Lee , D.D., Sugiyama,\\nM., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neural Infor-\\nmation Processing Systems 29, pp. 2234–2242. Curran Associ ates, Inc. (2016),\\nhttp://papers.nips.cc/paper/6125-improved-technique s-for-training-gans.pdf\\n29. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residuals\\nand linear bottlenecks: Mobile networks for classiﬁcation , detection and segmenta-\\ntion. CoRR abs/1801.04381 (2018),http://arxiv.org/abs/1801.04381\\n30. Shi, Z.: Mobilenet. https://github.com/Zehaos/MobileNet (2017)\\n31. Simonyan, K., Zisserman, A.: Very deep convolutional ne tworks\\nfor large-scale image recognition. CoRR abs/1409.1556 (2014),\\nhttp://arxiv.org/abs/1409.1556\\n32. Teti, R., Kumara, S.: Intelligent computing methods for manufacturing systems\\n46, 629–652 (12 1997)\\n33. Urban, G., Geras, K.J., Ebrahimi Kahou, S., Aslan, O., Wa ng, S., Caruana, R.,\\nMohamed, A., Philipose, M., Richardson, M.: Do Deep Convolu tional Nets Really\\nNeed to be Deep and Convolutional? ArXiv e-prints (Mar 2016)\\n34. Xu, Z., Hsu, Y.C., Huang, J.: Training shallow and thin ne tworks for accel-\\neration via knowledge distillation with conditional adver sarial networks (2018),\\nhttps://openreview.net/forum?id=BJbtuRRLM'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 16}, page_content='This figure \"kdgan_arch.png\" is available in \"png\"\\n format from:\\nhttp://arxiv.org/ps/1812.00660v1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('kd-ic.pdf')\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='arXiv:1812.00660v1  [cs.LG]  3 Dec 2018Knowledge Distillation with Feature Maps for\\nImage Classiﬁcation\\nWei-Chun Chen, Chia-Che Chang, Chien-Yu Lu, and Che-Rung Lee\\nNational Tsing Hua University, Taiwan\\n{meatybobby,chang810249,j19550713 }@gmail.com,\\ncherung@cs.nthu.edu.tw\\nAbstract. The model reduction problem that eases the computation\\ncosts and latency of complex deep learning architectures ha s received an\\nincreasing number of investigations owing to its importanc e in model de-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='ployment. One promising method is knowledge distillation ( KD), which\\ncreates afast-to-execute studentmodel to mimic alarge tea cher network.\\nIn this paper, we propose a method, called KDFM (Knowledge Di stil-\\nlation with Feature Maps), which improves the eﬀectiveness of KD by\\nlearning the feature maps from the teacher network. Two majo r tech-\\nniques used in KDFM are shared classiﬁer and generative adve rsarial\\nnetwork. Experimental results show that KDFM can use a four l ayers'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='CNN to mimic DenseNet-40 and use MobileNet to mimic DenseNet -100.\\nBoth student networks have less than 1% accuracy loss compar ing to\\ntheir teacher models for CIFAR-100 datasets. The student ne tworks are\\n2-6 times faster than their teacher models for inference, an d the model\\nsize of MobileNet is less than half of DenseNet-100’s.\\nKeywords: Knowledge Distillation ·Model Compression ·Generative\\nAdversarial Network.\\n1 Introduction'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='Adversarial Network.\\n1 Introduction\\nDeep learning has shown its capability of solving various computer visio n prob-\\nlems, such as image classiﬁcation [18] and object detection [5]. Its su ccess also\\nenables many related applications, such as self-driving cars[2], medic al diagnosis\\n[22], and intelligent manufacturing [32].\\nHowever,thestate-of-the-artdeeplearningmodelsusuallyhave largememory\\nfootprintsandrequireintensivecomputationalpower.Forinstan ce,VGGNet [31]'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='requires more than 100 million parameters and more than 15 giga ﬂoat ing-point-\\noperations (GFLOPs) to inference an image of 224 ×224resolution. It is diﬃcult\\nto deploy these models on some platforms with limited resources, suc h as mobile\\ndevices, or Internet of Things (IOT) devices. In addition, the infe rence time may\\nbe too long to satisfy the real-time requests of tasks.\\nMany methods have been proposed to reduce the computational c osts of'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='deep learning models during the inference time. For instance, the we ight quan-\\ntization method [14] reduces the network size by quantizing the net work param-\\neters. Structure pruning [7] is another example that removes the unnecessary'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='2 W. Chen et al.\\nparameters or channels of a trained convolutional neural networ k (CNN), and\\nthen ﬁne-tunes the model to gain higher accuracy. These method s have achieved\\ncompetitive accuracy with less model size comparing to those of orig inal models.\\nAlthough they can eﬀectively reduce the model sizes and inference time, their\\noperations are usually not matching the instructions of commodity a cceleration\\nhardware, such as GPU or TPU. As a result, the real performance gain of those'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='methods may not be signiﬁcant comparing to the original models with h ardware\\nacceleration.\\nAnother promising directions of model reduction is Knowledge Distillat ion\\n(KD) [10], whose idea is to train a student network to mimic the ability of a\\nteachermodel.Thestudentmodelisusuallysmallerorfaster-to-e xecutethanthe\\nteacher model. Hinton & Dean [10] coined the name of Knowledge Distilla tion\\n(KD). They trained student networks by the “soft target”, a mo dify softmax'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='function which can provide more information than the traditional so ftmax func-\\ntion. The experiment shows KD can improve the performance of a sin gle shallow\\nnetwork by distilling the knowledge in an ensemble model. Romero & Beng io\\n[27] extended the idea of KD and proposed FITNET. They trained th inner and\\ndeeper student networks by the “intermediate-level hint”, which is from the hid-\\nden layers of the teacher network, and the “soft target” to lear n the teacher'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='network. The results show that FITNET can use fewer parameter s to mimic the\\nteachernetwork.In [34], Xu &Huang proposedthe method that us es conditional\\nadversarial networks to make student networks learn the logits o f the teacher\\nnetworks.Their experimentsshowedthat it canfurther improvet he performance\\nof student models trained by traditional KD.\\nHowever,those KD methods only learn the logits ofteachermodels. They are'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='usually not powerful enough to make student models mimic all kinds of teacher\\nmodels well. They often need to customize the student models for sp eciﬁc archi-\\ntectures. In addition, as the deep models become more and more co mplicated,\\nthe eﬀectiveness of previous methods for knowledge distillation dec reases. One\\nexample is DenseNet [12], which connects all layers directly with each o ther,\\nand requires more computation in inference time. In our experiment s, the simple'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='CNN student models learned from previous methods cannot achieve the similar\\naccuracy as the teacher model.\\nIn this paper, we propose a method, called KDFM (Knowledge Distillatio n\\nwith Feature Maps), which learns the feature maps from the teach er model. For\\nthe application of image classiﬁcation, feature maps often provide m ore informa-\\ntion than logits. The feature maps in the last layer are used because they possess'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='the high level features of the input images, which are the most infor mative for\\nclassiﬁcation. KDFM utilizes two techniques to distill the knowledge of feature\\nmaps. First, it lets the teacher model and the student model shar e the classiﬁer.\\nThrough the training of the shared classiﬁer, the student model c an learn the\\nfeature maps from the teacher. Second, the idea of generative a dversarial net-\\nworks (GANs) [6] is used to improve the learning process. The featu re map in'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 1}, page_content='CNN is a special type of images. During the learning process, the disc rimina-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='KDFM 3\\ntor is forcing the student model (generator) to generate similar f eature maps to\\nthose of the teacher model (inputs of GANs).\\nAlthough the method could be generally applied to other types of net works,\\nwe employ the DenseNets as the teacher models to illustrate the idea and to\\ndemonstrate its eﬀectiveness in the experiments. Unlike FITNET [2 7] whose\\nstudent models are thin and deep, we let the student models be shallo w and fat,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='because such kind of networks are easier to be parallelized on moder n accelera-\\ntors, such as GPU.\\nWe validated the eﬀectiveness of KDFM using CIFAR-100 datasets [1 7]\\nand ImageNet datasets [4]. The ﬁrst experiment uses a simple stude nt network\\nwhich only contains 4 convolutional layers and a fully-connected laye r to mimic\\nDenseNet-40(DenseNet with 40layers)onCIFAR-100.The result showsthe stu-\\ndentmodelgeneratedbyKDFMhaslessthan1%accuracylossand2 timesfaster'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='inference time comparing to DenseNet-40, which is better than oth er methods.\\nThe second experiment trains the model of MobileNet [11], a state-o f-the-art\\nnetwork for mobile and embedded platforms, to mimic DenseNet-100 (DenseNet\\nwith 100 layers) on CIFAR-100. The results show that the student model is\\nmore than 6 times faster than DenseNet-100 in terms of inference time, with\\nonly half model size and less than 1% accuracy loss. The third experim ent uses'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='MobileNet v2 [29] to mimic ResNet-152 [9] on ImageNet, and the accur acy of\\nKDFM is better than other KD methods.\\nThe rest of paper is organized as follows. Section 2 gives a brief illustr ation of\\nknowledge distillation (KD) and generative adversarial networks (G ANs). Sec-\\ntion 3 introduces the design of KDFM to construct a student model. Section\\n4 shows the experimental results and the performance compariso n with other'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='methods. The conclusion and future work are presented in the last section.\\n2 Related Work\\n2.1 Knowledge Distillation\\nIn [1], Ba& Carunaaskedan interestingquestion,“DoDeep Nets Rea llyNeed to\\nbe Deep?” Their answer is that shallow nets can be trained to perfor m similarly\\ntocomplex,well-engineered,deeperconvolutionalmodels.Themet hod theyused\\nto train shallow networks is mimicking the teacher networks’ logits, t he value'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 2}, page_content='before the softmax activation. In 2017, authors presented mor e experimental\\nresults in [33].\\nHinton & Dean [10] generalized this idea as Knowledge Distillation (KD).\\nThe concept of knowledge distillation is to train a student network by a hard\\ntargetPHand a soft target PS:\\nPH(x) =softmax (x) (1)\\nPS(x,t) =softmax(x\\nt)\\n(2)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='4 W. Chen et al.\\nwherexare logits in a neural network, and tis a hyper-parameter, t >1, to\\nsoften the probability distribution over classes. A higher value of tcould provide\\nmore information.\\nLetxTbe the logitsofthe teachernetworkand xSbe the logitsofthe student\\nnetwork. The goal of student network is to optimize the loss funct ion\\nLKD=λLH+(1−λ)LS, (3)\\nwhere\\nLH=H(PH(xS),y)) and (4)\\nLS=H(PS(xS,t),PS(xT,t)) (5)\\nandyisground-truthlabel.Theytrainedshallownetworksbythe“softt arget”of'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='teachernetworks.KDsoftens the output ofthe softmax funct ion, providingmore\\ninformation than traditional softmax functions. The experiment in this paper\\nshows KD can improve the performance of a model by distilling the kno wledge\\nin an ensemble model into a single model.\\nRomero & Bengio [27] proposed FITNET, which extends the idea of KD by\\nusing “intermediate-level hints” from the hidden layersof the teac her network to'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='guide the student networks. They train thinner and deeper stude nt networks to\\nlearnthe intermediaterepresentationsand the soft targetofth e teachernetwork.\\nThe results show that the student network of FITNET can perfor m comparable\\nor even better than the teacher network with fewer parameters .\\n2.2 Generative Adversarial Networks\\nGenerative Adversarial Networks (GANs) have shown impressive r esults for un-\\nsupervised learning tasks, such as image generation [6], image synth esis [26], and'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='image super-resolution [19]. A GAN usually consists of two modules: a g ener-\\nator (G) and a discriminator (D). In a typical GAN model, the discrimin ator\\nlearns to distinguish real samples and fake results produced by the generator,\\nand the generator learns to create samples which can be judged as real ones by\\nthe discriminator.\\nMirza & Osindero [21] extended GANs to a conditional model by feedin g\\nextra information, such as class labels, to the generator and discr iminator. Chen'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='& Abbeel [3] proposed InfoGAN, an information-theoretic extens ion to GANs,\\nwhich is able to learn disentangle representation. Some studies [24,2 8,25] modify\\nthe discriminator to contain an auxiliary decoder network that can o utput class\\nlabels for training data.\\n2.3 DenseNet\\nHuang&Weinberger[12]proposedanewarchitecture,DenseNet ,whichconnects\\nall layers directly with each other. This idea is extended from ResNet [8] which'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 3}, page_content='aggregatespreviousfeature maps and feeds the summation into a layer.Diﬀerent'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='KDFM 5\\nfrom ResNet, DenseNet concatenates the feature maps from all preceding layers.\\nIt requires fewer parameters than traditional convolutional net works, because\\nit doesn’t need to relearn redundant feature maps. It performs s tate-of-the-art\\nresults on most classiﬁcation benchmark tasks.\\n2.4 MobileNet\\nHoward& Kalenichenko[11] proposedMobileNet for mobile and embedd ed plat-\\nforms. MobileNet uses depth-wise separable convolutions to reduc e the compu-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='tation and build a light-weight network. MobileNet allows to build the mod el\\non resource and accuracy trade-oﬀs by using width multiplier and re solution\\nmultiplier. The eﬀectiveness of MobileNet has been demonstrated ac ross a wide\\nrange of applications.\\n3 The Design of KDFM\\nKDFMuseaGANwithanauxiliarydecodernetworkthatcanoutputcla sslabels\\nfor training data. More speciﬁcally, it consists of three component s, a generator'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='G, a discriminator D, and a classiﬁer C. The generator Gis a feature extractor\\nwho produces the feature maps from the input images. The discrimin atorD\\ndistinguishes the real feature map, generated by the teacher ne twork, and the\\nfake feature map, generated by G. The classiﬁer Cis a feature decoder, whose\\ninputs arealso feature maps, and outputs are the hard target an d the soft target,\\nas deﬁned in (1) and (2).\\nThe goal of KDFM is to make Glearn the feature map from the teacher'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='network, and to train Cto classify the images based on the feature maps. Two\\nobjective functions, adversarialloss and knowledge distillation loss , are designed\\nto achieve the goal. The adversarial loss of KDFM is adopted from th e objective\\nfunction of LSGAN [20],\\nLadvD=1\\n2[D(G(X))]2+1\\n2[D(T(X))−1]2(6)\\nLadvG=1\\n2[D(G(X))−1]2(7)\\nwhereXdenotes the input images, G(X) is the feature maps generated by G,\\nandT(X) is the feature maps generated by the teacher model. The functio nDis'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 4}, page_content='designed to discriminate between the realfeature map T(X)and the fake feature\\nmapG(X). We chose LSGAN because it is the state-of-the-art GAN model\\nand the range of its loss function can be easily combined with the know ledge\\ndistillation loss.\\nThe knowledge distillation loss in KDFM is deﬁned as below:\\nLKD=λLH+(1−λ)LS (8)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='6 W. Chen et al.\\nwhere\\nLH=H(PH(C(G(X))),PH(z))+H(PH(C(T(X))),PH(z)) (9)\\nLS=H(PS(C(G(X)),t),PS(z,t))+H(PS(C(T(X)),t),PS(z,t)) (10)\\nthe value zis the logits from the teacher network, Hrefers to cross-entropy,and\\nλis a hyper-parameter, 0 < λ <1, controlling the ratio of LHandLS. If the\\nstudent model is similar to the teacher model, λneed not be large. Besides, we\\nchange the ground-truth label to the label of the teacher netwo rk in (9). The\\nexperiment also shows that it achieves better accuracy.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='Unlike traditional GAN, the loss function of Gin KDFM combines the ad-\\nversarial loss and the knowledge distillation loss,\\nLG=LadvG+αLKD (11)\\nwhereαis a hyper-parameter to balance the scale of the adversarial loss a nd the\\nknowledge distillation loss.\\nThe training of KDFM is to minimize the loss functions of three compone nts\\nsimultaneously. For the generator G, the loss function is LG, as deﬁned in (11);\\nfor the discriminator D, the loss function is LadvD, as deﬁned in (6); and for the'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='classiﬁer C, the loss function is LKD, deﬁned in (8).\\nInput  \\nimageTeacher  \\nnetwork\\nGReal\\nfeature  map\\nFake\\nfeature  mapD\\nCReal  /  Fake\\nHard  targetSoft  target\\nStudent  network\\nFig.1.Overview of KDFM, consisting of three module, a discriminat or D, a generator\\nG, andaclassiﬁer C. G andCcompose astudentnetwork. Thestu dentnetwork outputs\\nthe hard target for the inference.\\nFigure 1 shows the network architecture of KDFM. The student ne twork'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='consists of two parts, the feature extractor Gand the feature decoder C. The\\nfeature extractor generates the feature map, and the featur e decoder classiﬁes\\nthe feature map to probability distribution over classes. After eac h components\\narewell-trained,the student networkis constructedfrom GandC. In ourdesign,\\nConly has a pooling layer and one fully connected layer.\\nThe training process works like the alternative least square (ALS) m ethod.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 5}, page_content='Let’s use LHto illustrate the idea, since LShas the same structure. To minimize\\nH(PH(C(T(X))),PH(z)), the classiﬁer Cneeds to learn teacher network’s hard'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='KDFM 7\\ntargetPH(z). Meanwhile, the term LKDis also added to the loss function of the\\nstudent network. To minimize the H(PH(C(G(X))),PH(z)), the student model\\nmust output feature maps G(X) similar to T(X), so that PH(C(G(X))) can\\napproximate PH(z).\\n4 Experiments\\nWevalidatedtheeﬀectivenessofKDFMusingCIFAR-100andImageN etdatasets.\\nWe used DenseNet and ResNet as the teacher models, whose impleme ntations\\n[16] are in TensorFlow, and followed the standard training process w ith data'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='augmentation. Two types of student models are used in the experim ents. The\\nﬁrst kind of student models are simple convolutional neural networ ks (CNNs)\\nthat consist of several convolutional layers and one fully-connec ted layer, with\\nReLUactivation[23],batchnormalization[15],andmax-poolinglayers. Thecon-\\nvolutional layers are with 3 ×3 kernel size, and 64 to 1024 channels, depending\\non the parameter sizes. The second student model is MobileNet, wh ich has a'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='Tensorﬂow implementation [30] on Github. We modiﬁed the student mo dels so\\nthat the dimension ofstudent model’s feature maps equal to the te achermodel’s.\\nWithout further speciﬁcation, the hyper-parameter tandλ, as deﬁned in (2) and\\n(8), are set to 10 and 0 .1 respectively. The hyper-parameters α, deﬁned in (11),\\nis set to 10. The performance metrics of models are the accuracy a nd the infer-\\nence time, which is obtained from the average inference time of pred icting one'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='CIFAR-100 image 1000 times on one NVIDIA 1080Ti GPU.\\n4.1 Teacher Network: DenseNet-40\\nThis set of experiments uses various CNN models to mimic DenseNet-4 0. We\\ncompare the results of KDFM with other knowledge distillation method s, and\\njustify the inﬂuence of four factors to the accuracy and the infe rence time: the\\nnumber of layers, the number of parameters, the value of hyper- parameter tand\\nλ, deﬁned in (2) and (8).\\nComparison with other methods. We compared the accuracyof the student'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='network generated by KDFM and other two knowledge distillation met hods:\\nlogits mimic learning [1] and KD [10]. We also included the results of the mod el\\ntrained without any KD process as the baseline. The teacher model is DenseNet-\\n40 and the student model has 8 convolutional layers and 8 million train able\\nparameters. Table 1 shows the results of diﬀerent training method s. The result\\nindicates that the student model trained by KDFM can acheive similar accuracy'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 6}, page_content='as the teacher model’s. Logits mimic learning performs poorly. Its ac curacy is\\neven lower than that of the baseline in this case.\\nDiﬀerent number of layers. Table 2 summarizes setting of student and\\nteacher models, and their experimental results. There are four s tudent mod-\\nels which have 2, 4, 6, 8 convolutional layers respectively. We ﬁxed t he number'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='8 W. Chen et al.\\nMethod Accuracy\\nBaseline 68.53%\\nLogits Mimic Learning 50.95%\\nKD 69.14%\\nKDFM 74.10%\\nTeacher(DenseNet-40) 74.23%\\nTable 1. Testingaccuracy for training thestudentnetworks with 8co nvolutional layers\\nand 8M parameters by Baseline (typical training process), L ogits Mimic Learning, KD,\\nand KDFM.\\nof parameters to 8 millions. As can be seen, when the number of conv olution\\nlayersis largerthan4, the student models achievesimilaraccuracya sthe teacher'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='model. Although the model size is not small, the inference time of stud ent mod-\\nels is much shorter than that of the teach network. Particularly, t he student\\nnetwork with 4 convolution layers has better accuracy than the te acher model,\\nand its inference time is only half of the teacher model’s.\\nFigure2plots theaccuracyofstudent networksfordiﬀerentnum ber oflayers.\\nA clear trend is that when the number of layers is larger than 4, the s tudent'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='model can achieve similar accuracy as the teacher model. However, when the\\nnumber of layers is small, even with a large number of parameters, th e student\\nmodel cannotlearn well as the teacher model. This result matches t he conclusion\\nmade in [33].\\nModel No. Parameters Accuracy Inference time\\n2 conv ∼8M 59.19% 3.65ms\\n4 conv ∼8M 74.77% 2.46ms\\n6 conv ∼8M 74.08% 2.59ms\\n8 conv ∼8M 74.10% 2.75ms\\nDenseNet-40(Teacher) 1.1M 74.23% 5.28ms'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='DenseNet-40(Teacher) 1.1M 74.23% 5.28ms\\nTable 2. Testing accuracy and inference time for the student network s with 2, 4, 6,\\nand 8 convolutional layers mimicking DenseNet-40 by KDFM.\\nDiﬀerent number of parameters. Table 3 lists the setting and the results of\\nsix student models with diﬀerent number of parameters. The numbe r of layers\\nof CNNs is ﬁxed at 4, and the number of parameters are varied from 0.5M, 1M,\\n2M, 4M, 6M, to 8M. As can be seen, the more parameters, the bett er accuracy'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 7}, page_content='of the model. When the number of parameters is larger than or equa l to 4M, the\\naccuracy of student model is similar to that of the teacher model. T he diﬀerence\\nis less than 1%. Figure 3 shows this trend.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 8}, page_content='KDFM 9\\n2 3 4 5 6 7 8\\n# of conlolutional layers6062646668707274Accuracy(%)student\\nteacher\\nFig.2.Accuracy of student networks with diﬀerent convolution lay ers and 8 million\\nparameters, the horizontal line is the accuracy of the teach er network, DenseNet-40.\\nHowever,theinferencetimeofstudentmodelsisalsoincreasingast henumber\\nof parameter increases. Nevertheless, even when the number of parameter is 8M,\\nthe inference time is still less than half of the teacher model’s. The tr ade-oﬀ'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 8}, page_content='between accuracy and the inference time can be used to adjust th e student\\nmodels to ﬁt the requirements of deployments.\\nModel No. Parameters Accuracy Inference time\\n4conv-0.5M ∼0.5M 65.76% 1.61ms\\n4conv-1M ∼1M 67.83% 1.65ms\\n4conv-2M ∼2M 71.12% 1.73ms\\n4conv-4M ∼4M 73.77% 2.01ms\\n4conv-6M ∼6M 73.84% 2.32ms\\n4conv-8M ∼8M 74.77% 2.46ms\\nDenseNet-40(Teacher) 1.1M 74.23% 5.28ms\\nTable 3. Testing accuracy and inference time for the student network s with 4 convolu-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 8}, page_content='tional layers and diﬀerent numbers of parameters mimicking DenseNet-40 by KDFM.\\nDiﬀerent hyper-parameter t.We validated the inﬂuence of hyper-parameter\\nt, deﬁned in (2), to the accuracy of student models. Table 4 and Tab le 5 show\\nthe results for two models, one is a 4 layer CNNs with 2M parameters ( small\\nmodel), and the other is a 4 layer CNNs with 8M parameters (large mod el). As'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 9}, page_content='10 W. Chen et al.\\n12345678\\n# of trainable parameters(M)6668707274Accuracy(%)student\\nteacher\\nFig.3.Accuracy of student networks with 4 convolution layers and d iﬀerent number\\nof parameters, the horizontal line is the accuracy of the tea cher network, DenseNet-40.\\ncan be seen, the best result occurs at t= 5 for the small model and at t= 10 for\\nthe large model. This phenomenon can be reasoned as follows. When tis small,\\nthe soft target does not have enough relaxation to encourage st udent networks'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 9}, page_content='learning the teacher model. On the other hand, when tis too large, the teacher\\nmodel losses the disciplines to coach the student models. For weake r models,\\nsmallertcan usually give better accuracy, because they need clearer guide lines\\nto learn.\\nModel tAccuracy\\n4conv with t= 2 2 70.14%\\n4conv with t= 5 5 71.48%\\n4conv with t= 10 10 71.12%\\n4conv with t= 50 50 67.35%\\n4conv with t= 100 100 67.51%\\nDenseNet-40(Teacher) - 74.23%'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 9}, page_content='DenseNet-40(Teacher) - 74.23%\\nTable 4. Testing accuracy for the student networks with 4 convolutio nal layers, 2M\\nparameters, and diﬀerent hyper-paramter tmimicking DenseNet-40 by KDFM.\\nDiﬀerent hyper-parameter λ.This experiment compares the accuracy of\\nstudent models for diﬀerent hyper-parameter λ, deﬁned in (8). Table 11 and\\nTable 7 show the results for two models, one is a 4 layer CNNs with 2M pa ram-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 10}, page_content='KDFM 11\\nModel tAccuracy\\n4conv with t= 2 2 73.11%\\n4conv with t= 5 5 74.07%\\n4conv with t= 10 10 74.77%\\n4conv with t= 50 50 71.44%\\n4conv with t= 100 100 70.72%\\nDenseNet-40(Teacher) - 74.23%\\nTable 5. Testing accuracy for the student networks with 4 convolutio nal layers, 8M\\nparameters, and diﬀerent hyper-paramter tmimicking DenseNet-40 by KDFM.\\neters (small model), and the other is a 4 layer CNNs with 8M paramete rs (large\\nmodel). For both models, the best result occurs at λ= 0.1.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 10}, page_content='The results indicate the importance of soft target in knowledge dist illation.\\nForλ= 0.1, the value of soft target dominates the loss function of KD. This\\nshows that with more information, student models can learn better . However, if\\nλis set to 0, the information of hard target totally disappears, and t he student\\nmodel cannot learn the best results from the teacher model.\\nModel λAccuracy\\n4conv with λ= 0 0 70.87%\\n4conv with λ= 0.1 0.1 71.12%\\n4conv with λ= 0.4 0.4 67.30%'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 10}, page_content='4conv with λ= 0.4 0.4 67.30%\\n4conv with λ= 0.7 0.7 66.96%\\nDenseNet-40(Teacher) - 74.23%\\nTable 6. Testing accuracy for the student networks with 4 convolutio nal layers, 2M\\nparameters, and diﬀerent hyper-parameter λmimicking DenseNet-40 by KDFM.\\nModel λAccuracy\\n4conv with λ= 0 0 74.11%\\n4conv with λ= 0.1 0.1 74.77%\\n4conv with λ= 0.4 0.4 73.18%\\n4conv with λ= 0.7 0.7 71.68%\\nDenseNet-40(Teacher) - 74.23%\\nTable 7. Testing accuracy for the student networks with 4 convolutio nal layers, 8M'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 10}, page_content='parameters, and diﬀerent hyper-parameter λmimicking DenseNet-40 by KDFM.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='12 W. Chen et al.\\nDiﬀerent hyper-parameter α.The hyper-parameter αcontrols the ratio of\\nGAN and KD in generator’s loss function, LG=LadvG+αLKD. Table 8 lists\\nthe achieved accuracy of student model for diﬀerent α. The best result occurs\\natα= 10 in our experiments. If we take oﬀ GAN LadvG, as shown in the third\\nline, the accuracy also declines.\\nModel αAccuracy\\n4conv with α= 1 1 69.58%\\n4conv with α= 10 10 70.62%\\n4conv without LadvGLG=LKD69.57%\\nDenseNet-40(Teacher) - 74.23%'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='DenseNet-40(Teacher) - 74.23%\\nTable 8. Testing accuracy for the student networks with 4 convolutio nal layers, 6M\\nparameters, and diﬀerent hyper-parameter αmimicking DenseNet-40 by KDFM.\\n4.2 Teacher Network: DenseNet-100\\nSinceoneofthe goalsforknowledgedistillationistocreatemodelseas ytodeploy\\non small devices, in this experiment, we used MobileNet (student net work) to\\nmimic DenseNet-100 (teacher network). For comparison, we includ ed the results'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='of two other CNNs trained by KDFM. Both CNNs have 8 convolutional layers,\\nand one has 20.2M parameters; the other has 28.1M parameters. I n addition,\\nthe result of MobileNet, trained directly without KDFM, is also included as the\\nbaseline.\\nTable 9 summarizes the results. The ﬁrst three rows are the netwo rks trained\\nby KDFM, the fourth row is the result for MobileNet without KD, and t he last\\nrow is the result of DenseNet-100. As shown in the ﬁrst two rows, s imple CNNs,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='even with large amount of parameters, cannot achieve good accur acy as the\\nteacher model. But their inference times (4.56ms and 5.7ms) are muc h shorter\\nthan that of the original DenseNet-100 (18.02ms).\\nThe MobileNet trained by KDFM, as shown in the third row, has the bes t\\nresult in terms of model size, accuracy, and inference time. The nu mber of pa-\\nrameters of MobileNet (3.5M) is less than half of DenseNet-100’s (7.2 M), and'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='the inference time (2.79ms) is about 6 times faster than the original DenseNet-\\n100 (18.02ms). Comparing to the baseline, MobileNet without KD, the Mo-\\nbileNet trained by KDFM can achieve 77.20% accuracy, which is close to that\\nof DenseNet-100 (77.94%).\\n4.3 Teacher Network: CondenseNet\\nWe use CondenseNet-86 [13] (with stages [14, 14, 14] and growth [8 , 16, 32]) as\\nthe teacher network and a smaller CondenseNet-86 (with stages [1 4, 14, 14] and'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 11}, page_content='growth [8, 16, 16]) as the student network using CIFAR-100 datas et. The results\\nare shown in Table 10. The model trained by KDFM is improved.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 12}, page_content='KDFM 13\\nModel No.Parameters Accuracy Inference time\\n8 conv-20M (KDFM) 20.2M 74.36% 4.56ms\\n8 conv-28M (KDFM) 28.1M 75.25% 5.7ms\\nMobileNet (KDFM) 3.5M 77.20% 2.79ms\\nMobileNet(Baseline) 3.5M 72.99% 2.79ms\\nDenseNet-100(Teacher) 7.2M 77.94% 18.02ms\\nTable 9. Testing accuracy and inference time for training simple CNN s with 8 convo-\\nlutional layers and 20.2M, 28.1M parameters, and MobileNet as student networks by\\nKDFM.\\nModel No. Parameters FLOPs Accuracy'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 12}, page_content='KDFM.\\nModel No. Parameters FLOPs Accuracy\\nSmaller CondenseNet-86 (Baseline) 0.29M 49.95M 74.13%\\nSmaller CondenseNet-86 (KDFM) 0.29M 49.95M 75.01%\\nCondenseNet-86(Teacher) 0.55M 65.85M 76.02%\\nTable 10. Testing accuracy for the smaller CondenseNet-86 mimicking CondenseNet-\\n86 by KDFM and Baseline (typical training process) on CIFAR- 100 dataset.\\n4.4 ImageNet dataset\\nWe used MobileNet v2 as the student model to mimic the pre-trained R esNet-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 12}, page_content='152 using ImageNet dataset. Table 11 shows the experimental res ults of the\\ntesting accuracy using diﬀerent training methods for the student model. As can\\nbe seen, the baseline method (without KD) can only achieve 68.41% ac curacy.\\nThe KDFM model has the best result, 71.82%.\\nModel Accuracy Inference time FLOPs\\nMobileNet v2(KDFM) 71.82% 6ms 300M\\nMobileNet v2(KD) 70.16% 6ms 300M\\nMobileNet v2(KDFM without LadvG&LG=LKD) 71.32% 6ms 300M\\nMobileNet v2(Baseline) 68.01% 6ms 300M'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 12}, page_content='MobileNet v2(Baseline) 68.01% 6ms 300M\\nResNet-152(Pre-trained teacher) 78.31% 21ms 11G\\nTable 11. Testing accuracy for MobileNet v2 as the student networks, t rained by\\nKDFM, KD, KDFM without LadvG, and MobileNet v2 (baseline), to mimic ResNet-\\n152 by KDFM.\\n5 Conclusion and Future Work\\nWe presented a novel architecture, KDFM, which utilizes generativ e adversarial\\nnetworks to achieve knowledge distillation. The experiments demons trate that'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='14 W. Chen et al.\\nKDFM can use simple convolutional neural networks with shallower lay ers and\\nlarger number of trainable parameters to mimic state-of-the-art complicated\\nnetworks with comparable accuracy and faster inference time.\\nThe ideaofusing generativeadversarialnetworksfor knowledged istillation is\\nnot limited to the DenseNet or image classiﬁcation tasks, but can be g eneralized\\nto other types of networks for diﬀerent applications. It is also ort hogonal to'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='other model compression methods, which means one can use KDFM t o generate\\na student model and apply model pruning or other compression tec hniques to\\nfurther reduce the model size and improve the performance. Las t, what is the\\nbest student models to be used in KDFM still requires more investigat ions. One\\ngood feature of KDFM is that other objectives, such as model size , inference\\nspeed, power consumption, ﬁtting speciﬁc hardware, can be incor porated into\\nthe student model design.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='the student model design.\\nReferences\\n1. Ba, J., Caruana, R.: Do deep nets really need to be deep? In:\\nGhahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., We in-\\nberger, K.Q. (eds.) Advances in Neural Information Process -\\ning Systems 27, pp. 2654–2662. Curran Associates, Inc. (201 4),\\nhttp://papers.nips.cc/paper/5484-do-deep-nets-reall y-need-to-be-deep.pdf\\n2. Bojarski, M., Testa, D.D., Dworakowski, D., Firner, B., F lepp, B., Goyal, P.,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='Jackel, L.D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., Zieba,\\nK.: End to end learning for self-driving cars. CoRR abs/1604.07316 (2016),\\nhttp://arxiv.org/abs/1604.07316\\n3. Chen,X., Duan,Y., Houthooft, R.,Schulman,J., Sutskeve r,I., Abbeel,P.: Infogan:\\nInterpretable representation learning by information max imizing generative adver-\\nsarial nets. CoRR abs/1606.03657 (2016),http://arxiv.org/abs/1606.03657'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='4. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L .: ImageNet: A Large-\\nScale Hierarchical Image Database. In: CVPR09 (2009)\\n5. Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-\\ncurateobject detection andsemantic segmentation. CoRR abs/1311.2524 (2013),\\nhttp://arxiv.org/abs/1311.2524\\n6. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., War de-Farley, D., Ozair, S.,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='Courville, A.,Bengio, Y.:Generative adversarial nets. In :Ghahramani, Z., Welling,\\nM., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (eds.) Adv ances in Neural In-\\nformation Processing Systems 27, pp. 2672–2680. Curran Ass ociates, Inc. (2014),\\nhttp://papers.nips.cc/paper/5423-generative-adversa rial-nets.pdf\\n7. Hassibi, B., Stork, D.G.: Second order derivatives for ne twork pruning: Optimal\\nbrain surgeon. In: Hanson, S.J., Cowan, J.D., Giles, C.L. (e ds.) Advances in'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='Neural Information Processing Systems 5, pp. 164–171. Morg an-Kaufmann (1993),\\nhttp://papers.nips.cc/paper/647-second-order-deriva tives-for-network-pruning-optimal-brain-surgeon.pdf\\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: 2016 IEEE Conference on Computer Vision and Pattern Reco gnition (CVPR).\\npp. 770–778 (June 2016). https://doi.org/10.1109/CVPR.2 016.90\\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 13}, page_content='CoRRabs/1512.03385 (2015),http://arxiv.org/abs/1512.03385'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='KDFM 15\\n10. Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowle dge in a Neural Network.\\nArXiv e-prints (Mar 2015)\\n11. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand,\\nT., Andreetto, M., Adam, H.: Mobilenets: Eﬃcient convoluti onal neural\\nnetworks for mobile vision applications. CoRR abs/1704.04861 (2017),\\nhttp://arxiv.org/abs/1704.04861\\n12. Huang, G., Liu, Z., v. d. Maaten, L., Weinberger, K.Q.: De nsely con-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='nected convolutional networks. In: 2017 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR). pp. 2261–2269 (July 2017).\\nhttps://doi.org/10.1109/CVPR.2017.243\\n13. Huang, G., Liu, S., van der Maaten, L., Weinberger, K.Q.: Condensenet: An ef-\\nﬁcient densenet using learned group convolutions. In: The I EEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) (June 2018)\\n14. Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., B engio, Y.: Quantized neu-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='ral networks: Training neural networks with low precision w eights and activations.\\nCoRRabs/1609.07061 (2016),http://arxiv.org/abs/1609.07061\\n15. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerati ng deep network train-\\ning by reducing internal covariate shift. CoRR abs/1502.03167 (2015),\\nhttp://arxiv.org/abs/1502.03167\\n16. Khlestov, I.: vision networks. https://github.com/ikhlestov/vision_networks\\n(2017)\\n17. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='1(01 2009)\\n18. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet c lassiﬁcation with\\ndeep convolutional neural networks. In: Proceedings of the 25th Inter-\\nnational Conference on Neural Information Processing Syst ems - Vol-\\nume 1. pp. 1097–1105. NIPS’12, Curran Associates Inc., USA ( 2012),\\nhttp://dl.acm.org/citation.cfm?id=2999134.2999257\\n19. Ledig, C., Theis, L., Huszr, F., Caballero, J., Cunningh am, A., Acosta, A., Aitken,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='A., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-\\nresolution using a generative adversarial network. In: 201 7 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR). pp. 105–11 4 (July 2017).\\nhttps://doi.org/10.1109/CVPR.2017.19\\n20. Mao, X., Li, Q., Xie, H., Lau, R.Y.K., Wang, Z., Smolley, S .P.: Least squares gen-\\nerative adversarial networks. In: 2017 IEEE International Conference on Computer'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='Vision(ICCV).pp.2813–2821 (Oct2017). https://doi.org/ 10.1109/ICCV.2017.304\\n21. Mirza, M., Osindero, S.: Conditional generative advers arial nets. CoRR\\nabs/1411.1784 (2014),http://arxiv.org/abs/1411.1784\\n22. Mizotin, M., Benois-Pineau, J., Allard, M., Catheline, G.: Feature-based\\nbrain mri retrieval for alzheimer disease diagnosis. In: 20 12 19th IEEE In-\\nternational Conference on Image Processing. pp. 1241–1244 (Sept 2012).\\nhttps://doi.org/10.1109/ICIP.2012.6467091'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='https://doi.org/10.1109/ICIP.2012.6467091\\n23. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve re stricted boltzmann ma-\\nchines. In: Proceedings of the 27th International Conferen ce on International Con-\\nference on Machine Learning. pp. 807–814. ICML’10, Omnipre ss, USA (2010),\\nhttp://dl.acm.org/citation.cfm?id=3104322.3104425\\n24. Odena, A.: Semi-Supervised Learning with Generative Ad versarial Networks.\\nArXiv e-prints (Jun 2016)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 14}, page_content='ArXiv e-prints (Jun 2016)\\n25. Odena,A.,Olah,C.,Shlens,J.: Conditionalimage synth esiswithauxiliaryclassiﬁer\\nGANs. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34t h International Con-\\nference on Machine Learning. Proceedings of Machine Learni ng Research, vol. 70,'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='16 W. Chen et al.\\npp. 2642–2651. PMLR, International Convention Centre, Syd ney, Australia (06–11\\nAug 2017), http://proceedings.mlr.press/v70/odena17a.html\\n26. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B. , Lee, H.: Generative ad-\\nversarial text to image synthesis. In: Balcan, M.F., Weinbe rger, K.Q. (eds.) Pro-\\nceedings of The 33rd International Conference on Machine Le arning. Proceedings\\nof Machine Learning Research, vol. 48, pp. 1060–1069. PMLR, New York, New'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='York, USA (20–22 Jun 2016), http://proceedings.mlr.press/v48/reed16.html\\n27. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta , C., Bengio, Y.: Fitnets:\\nHints for thin deep nets. In: In Proceedings of ICLR (2015)\\n28. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., R adford, A., Chen,\\nX., Chen, X.: Improved techniques for training gans. In: Lee , D.D., Sugiyama,\\nM., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neural Infor-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='mation Processing Systems 29, pp. 2234–2242. Curran Associ ates, Inc. (2016),\\nhttp://papers.nips.cc/paper/6125-improved-technique s-for-training-gans.pdf\\n29. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residuals\\nand linear bottlenecks: Mobile networks for classiﬁcation , detection and segmenta-\\ntion. CoRR abs/1801.04381 (2018),http://arxiv.org/abs/1801.04381\\n30. Shi, Z.: Mobilenet. https://github.com/Zehaos/MobileNet (2017)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='31. Simonyan, K., Zisserman, A.: Very deep convolutional ne tworks\\nfor large-scale image recognition. CoRR abs/1409.1556 (2014),\\nhttp://arxiv.org/abs/1409.1556\\n32. Teti, R., Kumara, S.: Intelligent computing methods for manufacturing systems\\n46, 629–652 (12 1997)\\n33. Urban, G., Geras, K.J., Ebrahimi Kahou, S., Aslan, O., Wa ng, S., Caruana, R.,\\nMohamed, A., Philipose, M., Richardson, M.: Do Deep Convolu tional Nets Really\\nNeed to be Deep and Convolutional? ArXiv e-prints (Mar 2016)'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 15}, page_content='34. Xu, Z., Hsu, Y.C., Huang, J.: Training shallow and thin ne tworks for accel-\\neration via knowledge distillation with conditional adver sarial networks (2018),\\nhttps://openreview.net/forum?id=BJbtuRRLM'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 16}, page_content='This figure \"kdgan_arch.png\" is available in \"png\"\\n format from:\\nhttp://arxiv.org/ps/1812.00660v1')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_documents = text_splitter.split_documents(doc)\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='arXiv:1812.00660v1  [cs.LG]  3 Dec 2018Knowledge Distillation with Feature Maps for\\nImage Classiﬁcation\\nWei-Chun Chen, Chia-Che Chang, Chien-Yu Lu, and Che-Rung Lee\\nNational Tsing Hua University, Taiwan\\n{meatybobby,chang810249,j19550713 }@gmail.com,\\ncherung@cs.nthu.edu.tw\\nAbstract. The model reduction problem that eases the computation\\ncosts and latency of complex deep learning architectures ha s received an\\nincreasing number of investigations owing to its importanc e in model de-'),\n",
       " Document(metadata={'source': 'kd-ic.pdf', 'page': 0}, page_content='ployment. One promising method is knowledge distillation ( KD), which\\ncreates afast-to-execute studentmodel to mimic alarge tea cher network.\\nIn this paper, we propose a method, called KDFM (Knowledge Di stil-\\nlation with Feature Maps), which improves the eﬀectiveness of KD by\\nlearning the feature maps from the teacher network. Two majo r tech-\\nniques used in KDFM are shared classiﬁer and generative adve rsarial\\nnetwork. Experimental results show that KDFM can use a four l ayers'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_documents[0], final_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
