{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=ChatGroq(groq_api_key=groq_api_key,model_name=\"qwen-2.5-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"Give me the name, age and city of a fictional Person \\n {format_instruction}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the name, age and city of a fictional Person \n",
      " Return a JSON object.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Liam Chen', 'age': 32, 'city': 'New Angeles'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "f_result = parser.parse(response.content)\n",
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liam Chen'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_result['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Liam Chen', 'age': 32, 'city': 'New Avalon'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build a chain\n",
    "chain = template|model|parser\n",
    "result = chain.invoke({})\n",
    "# print(result.content)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Hare has problem like user can not provide what type of output user wants so llm decide what will be the out json format*\n",
    "\n",
    "- `StructuredOutputParse` is an output parser in LangChain that helps extract structured **JOSN** data form LLM response based on predefined field schemas.\n",
    "- There is a problem its can not do data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To build the schema use Response Schema\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the the topics'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the the topics'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the the topics'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Give 3 facts about the {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke(\n",
    "    {\n",
    "        'topic': \"Adaptive Prompting in LLMs\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n\\t\"fact_1\": \"Adaptive Prompting in Large Language Models (LLMs) involves dynamically adjusting the prompts based on the context and user input to improve the relevance and accuracy of the model\\'s responses.\",\\n\\t\"fact_2\": \"The technique of Adaptive Prompting can significantly enhance the performance of LLMs in specific domains or tasks by customizing the prompting strategy to better align with the nuances of the targeted application area.\",\\n\\t\"fact_3\": \"Implementing Adaptive Prompting requires sophisticated algorithms to analyze and understand the user\\'s intent and the context of the conversation, which can then be used to modify the prompts delivered to the LLM.\"\\n}\\n```', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 127, 'total_tokens': 267, 'completion_time': 0.7, 'prompt_time': 0.008348404, 'queue_time': 0.050822327, 'total_time': 0.708348404}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-88953bad-8528-4b1c-b9a6-5266fb74f2a6-0', usage_metadata={'input_tokens': 127, 'output_tokens': 140, 'total_tokens': 267})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': \"Adaptive Prompting in Large Language Models (LLMs) involves dynamically adjusting the prompts based on the context and user input to improve the relevance and accuracy of the model's responses.\", 'fact_2': 'The technique of Adaptive Prompting can significantly enhance the performance of LLMs in specific domains or tasks by customizing the prompting strategy to better align with the nuances of the targeted application area.', 'fact_3': \"Implementing Adaptive Prompting requires sophisticated algorithms to analyze and understand the user's intent and the context of the conversation, which can then be used to modify the prompts delivered to the LLM.\"}\n"
     ]
    }
   ],
   "source": [
    "final_res = parser.parse(result.content)\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The technique of Adaptive Prompting can significantly enhance the performance of LLMs in specific domains or tasks by customizing the prompting strategy to better align with the nuances of the targeted application area.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res['fact_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use chain\n",
    "chain = template | model | parser\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        'topic': \"Gen AI\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fact_1': 'Generative AI, or Gen AI, refers to artificial intelligence systems designed to create new digital content such as text, images, or sounds that can mimic the style and characteristics of human-created content.',\n",
       " 'fact_2': 'One of the most notable advancements in Gen AI is the development of large language models, like GPT (Generative Pre-trained Transformer), which are capable of generating human-like text based on input prompts.',\n",
       " 'fact_3': 'Gen AI technologies have a wide range of applications, from assisting in creative writing and artwork generation to simulating realistic voices for virtual assistants, but also raise ethical concerns regarding the potential for misuse, such as the creation of deepfakes and misinformation.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *PydanticOutputParser*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str = Field(description='Name of the Person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city should be in Bangladesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age, and city of the fictional {place} Person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | model | parser\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        'place': 'Bangladesh'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Rajib Alam', age=29, city='Dhaka')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['place'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"Name of the Person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"Age of the person\", \"exclusiveMinimum\": 18, \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"Name of the city should be in Bangladesh\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\\n```'}, template='Generate the name, age, and city of the fictional {place} Person \\n {format_instruction}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
