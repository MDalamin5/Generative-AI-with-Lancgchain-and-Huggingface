{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=ChatGroq(groq_api_key=groq_api_key,model_name=\"qwen-2.5-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template1 = PromptTemplate(\n",
    "    template=\"write a detailed report on {topics}\",\n",
    "    input_variables=['topics']\n",
    ")\n",
    "\n",
    "template2 = PromptTemplate(\n",
    "    template=\"write a 5 line summary on the following text. \\n {text}\",\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template1 | model | parser | template2 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Adaptive prompting improves Large Language Models' (LLMs) performance by dynamically adjusting input based on context, task requirements, and feedback. It enhances model flexibility, accuracy, and user satisfaction across various applications like customer service and content generation. However, it faces challenges such as complexity in implementation and potential overfitting. Future research aims to refine these techniques and address ethical concerns, promising significant advancements in LLM utility.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 1110, 'total_tokens': 1194, 'completion_time': 0.42, 'prompt_time': 0.053876903, 'queue_time': 0.09934580299999998, 'total_time': 0.473876903}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None} id='run-7ad70b11-d343-4c65-a387-261dc89f2c5e-0' usage_metadata={'input_tokens': 1110, 'output_tokens': 84, 'total_tokens': 1194}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        'topics': \"Adaptive Prompting in LLMs\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Adaptive prompting improves Large Language Models' (LLMs) performance by dynamically adjusting input based on context, task requirements, and feedback. It enhances model flexibility, accuracy, and user satisfaction across various applications like customer service and content generation. However, it faces challenges such as complexity in implementation and potential overfitting. Future research aims to refine these techniques and address ethical concerns, promising significant advancements in LLM utility.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
