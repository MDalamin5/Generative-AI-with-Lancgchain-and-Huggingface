{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableSequence\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model= ChatGroq(groq_api_key=groq_api_key,model_name=\"qwen-2.5-32b\")\n",
    "model_2 = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task is like generate tweet and linkedin post on same topics in parallel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template=\"Generate a tweet about {topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a linkedin post about this {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Now build the parallel chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        'tweet': RunnableSequence(prompt1, model, parser),\n",
    "        'linkedin': RunnableSequence(prompt2, model, parser)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': 'Exploring the endless possibilities with AI, where innovation meets intelligence to shape a future where the impossible becomes possible. ðŸŒŸðŸ¤– #ArtificialIntelligence #AITransformations #TechInnovation',\n",
       " 'linkedin': \"Exciting times ahead! I'm thrilled to share that Alibaba Cloud has recently launched Qwen, a state-of-the-art AI model designed to revolutionize the way we interact with technology. Qwen is not just another addition to the AI family; it's a leap forward in natural language processing, designed to assist, learn, and adapt to a wide array of tasks and interactions with unparalleled accuracy and efficiency.\\n\\nWith Qwen, we're setting new standards in AI interaction, aiming to make technology more accessible and intuitive for everyone. Whether you're looking to enhance your business processes, explore innovative solutions, or simply stay ahead in the tech game, Qwen is your partner for the future.\\n\\nStay tuned as we continue to unveil more features and applications of this groundbreaking AI. Let's shape the future together!\\n\\n#AIInnovation #AlibabaCloud #Qwen #TechRevolution #FutureOfAI\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke(\n",
    "    {\n",
    "        'topic': \"AI\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = parallel_chain.invoke(\n",
    "    {\n",
    "        'topic': \"machine Learning\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Unlocking the future with Machine Learning: where data meets destiny to create innovative solutions and drive change. #MachineLearning #TechInnovation\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +-------------------------------+          \n",
      "        | Parallel<tweet,linkedin>Input |          \n",
      "        +-------------------------------+          \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "        +--------------------------------+         \n",
      "        | Parallel<tweet,linkedin>Output |         \n",
      "        +--------------------------------+         \n"
     ]
    }
   ],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
