{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Explore RunnableLambda***\n",
    "* use runnableLambda you can do convert your custom function can be work as Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableSequence, RunnableParallel, RunnableLambda\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model= ChatGroq(groq_api_key=groq_api_key,model_name=\"qwen-2.5-32b\")\n",
    "model2= ChatGroq(groq_api_key=groq_api_key,model_name=\"llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How work Runnable Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a py function\n",
    "def wordCounter(text:str):\n",
    "    return len(text.split())\n",
    "\n",
    "## make Runnable Lambda\n",
    "runnable_word_counter =RunnableLambda(wordCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_word_counter.invoke(\"Hello sir how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Write a joke about this {topic}\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Explain the following joke {text}\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        'joke': RunnablePassthrough(),\n",
    "        'word_count': runnable_word_counter\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-------------+              \n",
      "              | PromptInput |              \n",
      "              +-------------+              \n",
      "                     *                     \n",
      "                     *                     \n",
      "                     *                     \n",
      "            +----------------+             \n",
      "            | PromptTemplate |             \n",
      "            +----------------+             \n",
      "                     *                     \n",
      "                     *                     \n",
      "                     *                     \n",
      "               +----------+                \n",
      "               | ChatGroq |                \n",
      "               +----------+                \n",
      "                     *                     \n",
      "                     *                     \n",
      "                     *                     \n",
      "            +-----------------+            \n",
      "            | StrOutputParser |            \n",
      "            +-----------------+            \n",
      "                     *                     \n",
      "                     *                     \n",
      "                     *                     \n",
      "    +--------------------------------+     \n",
      "    | Parallel<joke,word_count>Input |     \n",
      "    +--------------------------------+     \n",
      "              **           **              \n",
      "            **               **            \n",
      "          **                   **          \n",
      "+-------------+            +-------------+ \n",
      "| Passthrough |            | wordCounter | \n",
      "+-------------+            +-------------+ \n",
      "              **           **              \n",
      "                **       **                \n",
      "                  **   **                  \n",
      "    +---------------------------------+    \n",
      "    | Parallel<joke,word_count>Output |    \n",
      "    +---------------------------------+    \n"
     ]
    }
   ],
   "source": [
    "final_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = final_chain.invoke({\n",
    "    'topic': 'Laptop'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the laptop go to the doctor?\\n\\nBecause it had a virus and it felt a bit blue!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['joke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
